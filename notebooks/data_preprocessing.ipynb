{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunflowerseed17/PythonDSProject/blob/main/Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: praw in c:\\users\\jgber\\anaconda3\\lib\\site-packages (7.8.1)\n",
            "Requirement already satisfied: prawcore<3,>=2.4 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from praw) (2.4.0)\n",
            "Requirement already satisfied: update_checker>=0.18 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
            "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from praw) (0.58.0)\n",
            "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.4->praw) (2.28.1)\n",
            "Requirement already satisfied: six in c:\\users\\jgber\\appdata\\roaming\\python\\python310\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (2022.12.7)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (1.26.14)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\jgber\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.4->praw) (3.4)\n"
          ]
        }
      ],
      "source": [
        "# Importing dependencies\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "!pip install praw\n",
        "from datetime import datetime, timedelta\n",
        "import praw\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Scrape Reddit For Depression Posts (ONLY RUN ONCE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import praw\n",
        "\n",
        "# Configure Reddit API\n",
        "reddit = praw.Reddit(\n",
        "    client_id=\"ZaUY5qF9eLVVpD2OvHGEhg\",\n",
        "    client_secret=\"djHnirfkPnZUNI7XNs4dKUflOKjmtQ\",\n",
        "    user_agent=\"TextScraper by u/Jammberg\",\n",
        "    check_for_async=False\n",
        ")\n",
        "\n",
        "# List of related subreddits for specific scraping\n",
        "subreddits = [\"depression\", \"breastcancer\"]\n",
        "\n",
        "# Regex pattern for phrases indicating \"I have been diagnosed with\"\n",
        "search_pattern = re.compile(\n",
        "    r\"(i\\s+(was|am|have been|got|recently got|just got|was just|had been|found out i\\s+was|\"\n",
        "    r\"was diagnosed as having|diagnosed as suffering from|got diagnosed as having|received a diagnosis of|\"\n",
        "    r\"was told i\\s+have|was informed i\\s+have)\\s+.*)\",\n",
        "    re.IGNORECASE\n",
        ")\n",
        "\n",
        "# Function to create output folder\n",
        "def create_folder(folder_name):\n",
        "    if not os.path.exists(folder_name):\n",
        "        os.makedirs(folder_name)\n",
        "\n",
        "# Function to save a post to a file\n",
        "def save_post(post, output_folder, filename_prefix=\"post\"):\n",
        "    filename = f\"{filename_prefix}_{post.id}.txt\"\n",
        "    filepath = os.path.join(output_folder, filename)\n",
        "    try:\n",
        "        with open(filepath, \"w\", encoding=\"utf-8\") as file:\n",
        "            file.write(f\"Subreddit: {post.subreddit.display_name}\\n\")\n",
        "            file.write(f\"Title: {post.title}\\n\")\n",
        "            file.write(f\"Author: {post.author}\\n\")\n",
        "            file.write(f\"Score: {post.score}\\n\")\n",
        "            file.write(f\"Created UTC: {datetime.utcfromtimestamp(post.created_utc)}\\n\")\n",
        "            file.write(f\"URL: {post.url}\\n\")\n",
        "            file.write(\"\\n\")\n",
        "            file.write(post.selftext)\n",
        "        print(f\"Saved post to {filepath}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving post {post.id}: {e}\")\n",
        "\n",
        "# Function to fetch posts by a specific user\n",
        "def fetch_user_posts(username, reference_date, output_folder, max_posts=None):\n",
        "    if not username:\n",
        "        print(\"Author not available for this post.\")\n",
        "        return 0\n",
        "\n",
        "    post_count = 0\n",
        "    try:\n",
        "        user = reddit.redditor(username)\n",
        "        one_month_ago = reference_date - timedelta(days=30)\n",
        "        for post in user.submissions.new(limit=None):\n",
        "            if max_posts and post_count >= max_posts:\n",
        "                break\n",
        "            post_date = datetime.utcfromtimestamp(post.created_utc)\n",
        "            if one_month_ago <= post_date <= reference_date and post.selftext.strip():\n",
        "                save_post(post, output_folder, filename_prefix=\"user\")\n",
        "                post_count += 1\n",
        "            elif post_date < one_month_ago:\n",
        "                break\n",
        "            time.sleep(2)  # Avoid hitting rate limits\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching posts for user {username}: {e}\")\n",
        "\n",
        "    return post_count\n",
        "\n",
        "# Function to fetch posts from a specific subreddit\n",
        "def fetch_posts_from_subreddit(subreddit_name):\n",
        "    print(f\"\\nFetching posts from r/{subreddit_name}...\\n\")\n",
        "    subreddit = reddit.subreddit(subreddit_name)\n",
        "    output_folder = f\"reddit_scraped_posts/{subreddit_name}\"\n",
        "    create_folder(output_folder)\n",
        "\n",
        "    try:\n",
        "        for post in subreddit.new(limit=None):\n",
        "            if post.selftext.strip() and re.search(search_pattern, post.selftext):\n",
        "                reference_date = datetime.utcfromtimestamp(post.created_utc)\n",
        "                save_post(post, output_folder, filename_prefix=\"subreddit\")\n",
        "                fetch_user_posts(post.author.name, reference_date, output_folder)\n",
        "            time.sleep(2)  # Avoid hitting rate limits\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching posts from r/{subreddit_name}: {e}\")\n",
        "\n",
        "# Function to fetch posts from r/all (new) within the last 2 months or until a maximum number of posts\n",
        "def fetch_posts_from_all(max_posts=1100, max_retries=3):\n",
        "    print(\"\\nFetching posts from r/all (new)...\\n\")\n",
        "    subreddit = reddit.subreddit(\"all\")\n",
        "    output_folder = \"data/reddit_scraped_posts/standard\"\n",
        "    create_folder(output_folder)\n",
        "\n",
        "    cutoff_date = datetime.utcnow() - timedelta(days=60)\n",
        "    post_count = 0\n",
        "    attempts = 0\n",
        "\n",
        "    while attempts < max_retries:\n",
        "        try:\n",
        "            for post in subreddit.new(limit=None):\n",
        "                if post_count >= max_posts:\n",
        "                    break\n",
        "                post_date = datetime.utcfromtimestamp(post.created_utc)\n",
        "                if post_date < cutoff_date:\n",
        "                    break\n",
        "                if post.selftext.strip():\n",
        "                    save_post(post, output_folder, filename_prefix=\"all\")\n",
        "                    post_count += 1\n",
        "\n",
        "                    # Fetch additional posts from the same author\n",
        "                    if post.author:\n",
        "                        post_count += fetch_user_posts(post.author.name, post_date, output_folder, max_posts - post_count)\n",
        "\n",
        "            break  # Exit the retry loop if successful\n",
        "        except praw.exceptions.APIException as e:\n",
        "            print(f\"Rate limit exceeded: {e}. Retrying after a delay...\")\n",
        "            time.sleep(60)\n",
        "            attempts += 1\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching posts: {e}. Retrying...\")\n",
        "            time.sleep(30)\n",
        "            attempts += 1\n",
        "\n",
        "    print(f\"\\nTotal posts scraped: {post_count}\")\n",
        "\n",
        "# Main scraping logic\n",
        "if __name__ == \"__main__\":\n",
        "    for subreddit_name in subreddits:\n",
        "        fetch_posts_from_subreddit(subreddit_name)\n",
        "\n",
        "    fetch_posts_from_all()\n",
        "\n",
        "    print(\"\\nScraping complete! Text files saved in the respective folders.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHEMPcI17ZNX"
      },
      "source": [
        "### Preprocessing.\n",
        "#### The Code Below Does the Following:\n",
        "1. Tokenization: Splitting text into individual tokens (words or punctuation marks).\n",
        "2. Removing Noise: Cleaning the text by removing: URLs, Punctuation, and Stop words\n",
        "3. Stemming: Reducing words to their root form (e.g., \"running\" -> \"run\").\n",
        "4. Converting text to lowercase for consistency."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Contents of data\\reddit_scraped_posts\\depression:\n",
            "[]\n",
            "\n",
            "Contents of data\\reddit_scraped_posts\\breastcancer:\n",
            "[]\n",
            "\n",
            "Contents of data\\reddit_scraped_posts\\standard:\n",
            "[]\n"
          ]
        }
      ],
      "source": [
        "for category, paths in folders.items():\n",
        "    input_folder = paths[\"input\"]\n",
        "    if os.path.exists(input_folder):\n",
        "        print(f\"\\nContents of {input_folder}:\")\n",
        "        print(os.listdir(input_folder))\n",
        "    else:\n",
        "        print(f\"\\nInput folder does not exist: {input_folder}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing depression posts...\n",
            "\n",
            "Processing breastcancer posts...\n",
            "\n",
            "Processing standard posts...\n",
            "\n",
            "Preprocessing complete! Preprocessed files are saved in the respective output folders.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\jgber\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\jgber\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download NLTK data files (only the first time)\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Initialize tools\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Define folders for input and output\n",
        "folders = {\n",
        "    \"depression\": {\n",
        "        \"input\": r\"data\\reddit_scraped_posts\\depression\",\n",
        "        \"output\": r\"data\\preprocessed_posts\\depression\"\n",
        "    },\n",
        "    \"breastcancer\": {\n",
        "        \"input\": r\"data\\reddit_scraped_posts\\breastcancer\",\n",
        "        \"output\": r\"data\\preprocessed_posts\\breastcancer\"\n",
        "    },\n",
        "    \"standard\": {\n",
        "        \"input\": r\"data\\reddit_scraped_posts\\standard\",\n",
        "        \"output\": r\"data\\preprocessed_posts\\standard\"\n",
        "    }\n",
        "}\n",
        "\n",
        "# Ensure both input and output folders exist\n",
        "for category, paths in folders.items():\n",
        "    os.makedirs(paths[\"input\"], exist_ok=True)  # Create input folder if missing\n",
        "    os.makedirs(paths[\"output\"], exist_ok=True)  # Create output folder if missing\n",
        "\n",
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Preprocess the given text:\n",
        "    - Tokenize into words\n",
        "    - Lowercase and remove non-alphanumeric tokens\n",
        "    - Remove stop words\n",
        "    - Apply stemming\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Tokenize text\n",
        "        tokens = word_tokenize(text)\n",
        "        \n",
        "        # Remove URLs, punctuation, and stop words; lowercase the text\n",
        "        tokens = [\n",
        "            word.lower() for word in tokens \n",
        "            if word.isalnum() and word.lower() not in stop_words\n",
        "        ]\n",
        "        \n",
        "        # Apply stemming\n",
        "        tokens = [stemmer.stem(word) for word in tokens]\n",
        "        \n",
        "        # Join tokens back into a single string\n",
        "        return \" \".join(tokens)\n",
        "    except Exception as e:\n",
        "        print(f\"Error preprocessing text: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Process each category\n",
        "for category, paths in folders.items():\n",
        "    input_folder = paths[\"input\"]\n",
        "    output_folder = paths[\"output\"]\n",
        "    \n",
        "    if not os.path.exists(input_folder):\n",
        "        print(f\"Input folder does not exist: {input_folder}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\nProcessing {category} posts...\")\n",
        "    \n",
        "    # Process each file in the input folder\n",
        "    for filename in os.listdir(input_folder):\n",
        "        if filename.endswith(\".txt\"):\n",
        "            input_filepath = os.path.join(input_folder, filename)\n",
        "            output_filepath = os.path.join(output_folder, filename)\n",
        "            \n",
        "            try:\n",
        "                with open(input_filepath, \"r\", encoding=\"utf-8\") as infile:\n",
        "                    text = infile.read()\n",
        "                \n",
        "                # Extract post content (everything after the first header section)\n",
        "                post_content = \"\\n\".join(text.splitlines()[6:]).strip()\n",
        "                \n",
        "                # Skip files that are empty or contain '[removed]'\n",
        "                if not post_content or '[removed]' in post_content.lower():\n",
        "                    print(f\"Skipping {filename} (empty or contains '[removed]')\")\n",
        "                    continue\n",
        "                \n",
        "                # Preprocess the text\n",
        "                preprocessed_text = preprocess_text(post_content)\n",
        "                \n",
        "                # Save preprocessed text to a new file\n",
        "                with open(output_filepath, \"w\", encoding=\"utf-8\") as outfile:\n",
        "                    outfile.write(preprocessed_text)\n",
        "                \n",
        "                print(f\"Processed {filename} into {output_folder}\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing file {filename}: {e}\")\n",
        "\n",
        "print(\"\\nPreprocessing complete! Preprocessed files are saved in the respective output folders.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyORK9nQ1oLEnAczLDLGQ2xF",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
