{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing dependencies \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the preprocessed data as 'posts' and 'labels' to be used also\n",
    "\n",
    "models = []\n",
    "posts = []  \n",
    "labels = [] \n",
    "\n",
    "folders = {\n",
    "    \"depression\": {\n",
    "        \"path\": \"data/preprocessed_posts/depression\",\n",
    "        \"label\": 1  # Label for depression-related posts\n",
    "    },\n",
    "    \"breastcancer\": {\n",
    "        \"path\": \"data/preprocessed_posts/breastcancer\",\n",
    "        \"label\": 0  # Label for breast cancer posts\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, data in folders.items():\n",
    "    folder_path = data[\"path\"]\n",
    "    label = data[\"label\"]\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()  # Read the file content\n",
    "                posts.append(content)  # Add to postst list\n",
    "                labels.append(label)  # Add corresponding label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the feature extraction data\n",
    "empath_file = \"data/feature_extracted_data/empath_features_with_labels.csv\"\n",
    "lda_file = \"data/feature_extracted_data/lda_topic_distributions_with_labels.csv\"\n",
    "unigram_file = \"data/feature_extracted_data/unigram_features_with_labels.csv\"\n",
    "bigram_file = \"data/feature_extracted_data/bigram_features_with_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ModelTrainer class\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, csv_files, model, model_name, model_params=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the ModelTrainer class.\n",
    "\n",
    "        Parameters:\n",
    "        csv_files (list of str): List of file paths for the feature datasets (CSV files).\n",
    "        model (class): Machine learning model class (e.g., LogisticRegression, SVC).\n",
    "        model_name (str): Name of the model for identification.\n",
    "        model_params (dict): Parameters for the model.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model\n",
    "        self.model_params = model_params if model_params else {}\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.random_state = random_state\n",
    "        self.metrics = {}\n",
    "\n",
    "    def load_and_combine_data(self):\n",
    "        \"\"\"\n",
    "        Load and combine data from multiple CSV files into a single dataset.\n",
    "        Assumes each CSV has the same 'label' column.\n",
    "        \"\"\"\n",
    "        data_frames = [pd.read_csv(file) for file in self.csv_files]\n",
    "\n",
    "        # Ensure labels are consistent and take from the first dataset\n",
    "        labels = data_frames[0]['label']\n",
    "        for df in data_frames[1:]:\n",
    "            if 'label' in df.columns:\n",
    "                df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "        combined_data = pd.concat(data_frames, axis=1)\n",
    "\n",
    "        # Check alignment between features and labels\n",
    "        if len(labels) != len(combined_data):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between features and labels: \"\n",
    "                f\"{len(combined_data)} rows in features, {len(labels)} in labels.\"\n",
    "            )\n",
    "\n",
    "        # Add the label column\n",
    "        self.data = combined_data\n",
    "        self.data['label'] = labels\n",
    "\n",
    "    def preprocess_data(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "        test_size (float): Proportion of data to use for testing.\n",
    "        \"\"\"\n",
    "        X = self.data.iloc[:, :-1]  # All columns except the label column\n",
    "        y = self.data['label']  # Label column\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train the specified model on the training data.\n",
    "        \"\"\"\n",
    "        self.model = self.model_class(**self.model_params)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(f\"Model {self.model_name} trained successfully.\")\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test data and print metrics, including 10-fold cross-validation.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(f\"Model {self.model_name} has not been trained yet.\")\n",
    "\n",
    "        # Predictions and metrics for the test set\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        precision = precision_score(self.y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(self.y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        cv_scores = cross_val_score(self.model, self.X_train, self.y_train, cv=10, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "\n",
    "\n",
    "        # Store metrics\n",
    "        self.metrics = {\n",
    "            \"Model\": self.model_name,\n",
    "            \"Test Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"CV Mean Accuracy\": cv_mean,\n",
    "            \"CV Std Dev\": cv_std\n",
    "        }\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Complete pipeline: load data, preprocess, train, and evaluate.\n",
    "        \"\"\"\n",
    "        self.load_and_combine_data()\n",
    "        self.preprocess_data()\n",
    "        self.train_model()\n",
    "        self.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for compiling metrics of the models\n",
    "\n",
    "svm_models = []\n",
    "mlp_models = []\n",
    "lr_models = []\n",
    "rf_models = []\n",
    "ada_models = []\n",
    "models = []\n",
    "\n",
    "def compile_metrics(models):\n",
    "    metrics_data = [model.metrics for model in models]\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustable hyperparameters and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"AdaBoost\",\n",
    "        \"model_class\": AdaBoostClassifier,\n",
    "        \"params\": {'n_estimators': 50, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVM\",\n",
    "        \"model_class\": SVC,\n",
    "        \"params\": {'C': 1.0, 'kernel': 'linear', 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model_class\": RandomForestClassifier,\n",
    "        \"params\": {'n_estimators': 100, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"model_class\": LogisticRegression,\n",
    "        \"params\": {'max_iter': 500, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLP\",\n",
    "        \"model_class\": MLPClassifier,\n",
    "        \"params\": {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'random_state': 42}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature combinations\n",
    "feature_combinations = [\n",
    "    {\"files\": [empath_file], \"name_suffix\": \"(Empath)\"},\n",
    "    {\"files\": [lda_file], \"name_suffix\": \"(LDA)\"},\n",
    "    {\"files\": [unigram_file], \"name_suffix\": \"(unigram)\"},\n",
    "    {\"files\": [bigram_file], \"name_suffix\": \"(bigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, unigram_file], \"name_suffix\": \"(EM + LDA + unigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, bigram_file], \"name_suffix\": \"(EM + LDA + bigram)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training AdaBoost (Empath)...\n",
      "Model AdaBoost (Empath) trained successfully.\n",
      "\n",
      "Training AdaBoost (LDA)...\n",
      "Model AdaBoost (LDA) trained successfully.\n",
      "\n",
      "Training AdaBoost (unigram)...\n",
      "Model AdaBoost (unigram) trained successfully.\n",
      "\n",
      "Training AdaBoost (bigram)...\n",
      "Model AdaBoost (bigram) trained successfully.\n",
      "\n",
      "Training AdaBoost (EM + LDA + unigram)...\n",
      "Model AdaBoost (EM + LDA + unigram) trained successfully.\n",
      "\n",
      "Training AdaBoost (EM + LDA + bigram)...\n",
      "Model AdaBoost (EM + LDA + bigram) trained successfully.\n",
      "\n",
      "Training SVM (Empath)...\n",
      "Model SVM (Empath) trained successfully.\n",
      "\n",
      "Training SVM (LDA)...\n",
      "Model SVM (LDA) trained successfully.\n",
      "\n",
      "Training SVM (unigram)...\n",
      "Model SVM (unigram) trained successfully.\n",
      "\n",
      "Training SVM (bigram)...\n",
      "Model SVM (bigram) trained successfully.\n",
      "\n",
      "Training SVM (EM + LDA + unigram)...\n",
      "Model SVM (EM + LDA + unigram) trained successfully.\n",
      "\n",
      "Training SVM (EM + LDA + bigram)...\n",
      "Model SVM (EM + LDA + bigram) trained successfully.\n",
      "\n",
      "Training Random Forest (Empath)...\n",
      "Model Random Forest (Empath) trained successfully.\n",
      "\n",
      "Training Random Forest (LDA)...\n",
      "Model Random Forest (LDA) trained successfully.\n",
      "\n",
      "Training Random Forest (unigram)...\n",
      "Model Random Forest (unigram) trained successfully.\n",
      "\n",
      "Training Random Forest (bigram)...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_model_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m trainer \u001b[38;5;241m=\u001b[39m ModelTrainer(feature_files, model_class, full_model_name, model_params)\n\u001b[0;32m---> 20\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Store the model and its results\u001b[39;00m\n\u001b[1;32m     23\u001b[0m all_models\u001b[38;5;241m.\u001b[39mappend(trainer)\n",
      "Cell \u001b[0;32mIn[4], line 111\u001b[0m, in \u001b[0;36mModelTrainer.run_pipeline\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_pipeline\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    108\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;124;03m    Complete pipeline: load data, preprocess, train, and evaluate.\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_and_combine_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess_data()\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_model()\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36mModelTrainer.load_and_combine_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_combine_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Load and combine data from multiple CSV files into a single dataset.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Assumes each CSV has the same 'label' column.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     data_frames \u001b[38;5;241m=\u001b[39m [pd\u001b[38;5;241m.\u001b[39mread_csv(file) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcsv_files]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Ensure labels are consistent and take from the first dataset\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     labels \u001b[38;5;241m=\u001b[39m data_frames[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[4], line 33\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_and_combine_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     29\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;124;03m    Load and combine data from multiple CSV files into a single dataset.\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;124;03m    Assumes each CSV has the same 'label' column.\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     data_frames \u001b[38;5;241m=\u001b[39m [\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcsv_files]\n\u001b[1;32m     35\u001b[0m     \u001b[38;5;66;03m# Ensure labels are consistent and take from the first dataset\u001b[39;00m\n\u001b[1;32m     36\u001b[0m     labels \u001b[38;5;241m=\u001b[39m data_frames[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m    900\u001b[0m     dialect,\n\u001b[1;32m    901\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m    909\u001b[0m )\n\u001b[1;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:583\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m parser:\n\u001b[0;32m--> 583\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparser\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/io/parsers/readers.py:1704\u001b[0m, in \u001b[0;36mTextFileReader.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m   1697\u001b[0m nrows \u001b[38;5;241m=\u001b[39m validate_integer(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows\u001b[39m\u001b[38;5;124m\"\u001b[39m, nrows)\n\u001b[1;32m   1698\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1699\u001b[0m     \u001b[38;5;66;03m# error: \"ParserBase\" has no attribute \"read\"\u001b[39;00m\n\u001b[1;32m   1700\u001b[0m     (\n\u001b[1;32m   1701\u001b[0m         index,\n\u001b[1;32m   1702\u001b[0m         columns,\n\u001b[1;32m   1703\u001b[0m         col_dict,\n\u001b[0;32m-> 1704\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[attr-defined]\u001b[39;49;00m\n\u001b[1;32m   1705\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnrows\u001b[49m\n\u001b[1;32m   1706\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1707\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1708\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/io/parsers/c_parser_wrapper.py:234\u001b[0m, in \u001b[0;36mCParserWrapper.read\u001b[0;34m(self, nrows)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    233\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlow_memory:\n\u001b[0;32m--> 234\u001b[0m         chunks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_low_memory\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnrows\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# destructive to chunks\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         data \u001b[38;5;241m=\u001b[39m _concatenate_chunks(chunks)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:814\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.read_low_memory\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:891\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1036\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_column_data\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1090\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_tokens\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/_libs/parsers.pyx:1165\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader._convert_with_dtype\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/pandas/core/dtypes/common.py:1335\u001b[0m, in \u001b[0;36mis_extension_array_dtype\u001b[0;34m(arr_or_dtype)\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;66;03m# Note: if other EA dtypes are ever held in HybridBlock, exclude those\u001b[39;00m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;66;03m#  here too.\u001b[39;00m\n\u001b[1;32m   1328\u001b[0m     \u001b[38;5;66;03m# NB: need to check DatetimeTZDtype and not is_datetime64tz_dtype\u001b[39;00m\n\u001b[1;32m   1329\u001b[0m     \u001b[38;5;66;03m#  to exclude ArrowTimestampUSDtype\u001b[39;00m\n\u001b[1;32m   1330\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, ExtensionDtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[1;32m   1331\u001b[0m         dtype, (DatetimeTZDtype, PeriodDtype)\n\u001b[1;32m   1332\u001b[0m     )\n\u001b[0;32m-> 1335\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_extension_array_dtype\u001b[39m(arr_or_dtype) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m   1336\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1337\u001b[0m \u001b[38;5;124;03m    Check if an object is a pandas extension array type.\u001b[39;00m\n\u001b[1;32m   1338\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1378\u001b[0m \u001b[38;5;124;03m    False\u001b[39;00m\n\u001b[1;32m   1379\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m   1380\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(arr_or_dtype, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, arr_or_dtype)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "all_models = []\n",
    "model_specific_results = {}\n",
    "\n",
    "\n",
    "for model_config in models_config:\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_class = model_config[\"model_class\"]\n",
    "    model_params = model_config[\"params\"]\n",
    "    \n",
    "    # For each feature combination\n",
    "    specific_models = []\n",
    "    for feature_combo in feature_combinations:\n",
    "        feature_files = feature_combo[\"files\"]\n",
    "        name_suffix = feature_combo[\"name_suffix\"]\n",
    "        full_model_name = f\"{model_name} {name_suffix}\"\n",
    "        \n",
    "        print(f\"\\nTraining {full_model_name}...\")\n",
    "        trainer = ModelTrainer(feature_files, model_class, full_model_name, model_params)\n",
    "        trainer.run_pipeline()\n",
    "        \n",
    "        # Store the model and its results\n",
    "        all_models.append(trainer)\n",
    "        specific_models.append(trainer)\n",
    "    \n",
    "    # Compile metrics for the specific model type\n",
    "    model_specific_results[model_name] = compile_metrics(specific_models)\n",
    "\n",
    "# Compile overall metrics for all models\n",
    "overall_metrics_table = compile_metrics(all_models)\n",
    "print(\"\\nOverall Metrics Table:\")\n",
    "print(overall_metrics_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
