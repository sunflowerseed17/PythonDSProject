{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing dependencies \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the preprocessed data as 'posts' and 'labels' to be used also\n",
    "\n",
    "models = []\n",
    "posts = []  \n",
    "labels = [] \n",
    "\n",
    "folders = {\n",
    "    \"depression\": {\n",
    "        \"path\": \"data/preprocessed/preprocessed_depression_posts\",\n",
    "        \"label\": 1  # Label for depression-related posts\n",
    "    },\n",
    "    \"breastcancer\": {\n",
    "        \"path\": \"data/preprocessed/preprocessed_breastcancer_posts\",\n",
    "        \"label\": 0  # Label for breast cancer posts\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, data in folders.items():\n",
    "    folder_path = data[\"path\"]\n",
    "    label = data[\"label\"]\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()  # Read the file content\n",
    "                posts.append(content)  # Add to postst list\n",
    "                labels.append(label)  # Add corresponding label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the feature extraction data\n",
    "empath_file = \"data/feature_extracted_data/empath_features_with_labels.csv\"\n",
    "lda_file = \"data/feature_extracted_data/lda_topic_distributions_with_labels.csv\"\n",
    "unigram_file = \"data/feature_extracted_data/unigram_features_with_labels.csv\"\n",
    "bigram_file = \"data/feature_extracted_data/bigram_features_with_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ModelTrainer class\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, csv_files, model, model_name, model_params=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the ModelTrainer class.\n",
    "\n",
    "        Parameters:\n",
    "        csv_files (list of str): List of file paths for the feature datasets (CSV files).\n",
    "        model (class): Machine learning model class (e.g., LogisticRegression, SVC).\n",
    "        model_name (str): Name of the model for identification.\n",
    "        model_params (dict): Parameters for the model.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model\n",
    "        self.model_params = model_params if model_params else {}\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.random_state = random_state\n",
    "        self.metrics = {}\n",
    "\n",
    "    def load_and_combine_data(self):\n",
    "        \"\"\"\n",
    "        Load and combine data from multiple CSV files into a single dataset.\n",
    "        Assumes each CSV has the same 'label' column.\n",
    "        \"\"\"\n",
    "        print(f\"Loading data for {self.model_name}...\")\n",
    "        data_frames = [pd.read_csv(file) for file in self.csv_files]\n",
    "\n",
    "        # Ensure labels are consistent and take from the first dataset\n",
    "        labels = data_frames[0]['label']\n",
    "        for df in data_frames[1:]:\n",
    "            if 'label' in df.columns:\n",
    "                df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "        combined_data = pd.concat(data_frames, axis=1)\n",
    "\n",
    "        # Check alignment between features and labels\n",
    "        if len(labels) != len(combined_data):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between features and labels: \"\n",
    "                f\"{len(combined_data)} rows in features, {len(labels)} in labels.\"\n",
    "            )\n",
    "\n",
    "        # Add the label column\n",
    "        self.data = combined_data\n",
    "        self.data['label'] = labels\n",
    "        print(f\"Loaded data shape: {self.data.shape}\")\n",
    "\n",
    "    def preprocess_data(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "        test_size (float): Proportion of data to use for testing.\n",
    "        \"\"\"\n",
    "        print(\"Splitting data into train and test sets...\")\n",
    "        X = self.data.iloc[:, :-1]  # All columns except the label column\n",
    "        y = self.data['label']  # Label column\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state\n",
    "        )\n",
    "        print(f\"Training set size: {self.X_train.shape}, Test set size: {self.X_test.shape}\")\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train the specified model on the training data.\n",
    "        \"\"\"\n",
    "        print(f\"Training {self.model_name}...\")\n",
    "        self.model = self.model_class(**self.model_params)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(f\"Model {self.model_name} trained successfully.\")\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test data and print metrics.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(f\"Model {self.model_name} has not been trained yet.\")\n",
    "\n",
    "        print(f\"Evaluating {self.model_name}...\")\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        print(f\"Accuracy: {accuracy_score(self.y_test, y_pred)}\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        precision = precision_score(self.y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(self.y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Store metrics\n",
    "        self.metrics = {\n",
    "            \"Model\": self.model_name,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall\n",
    "        }\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Complete pipeline: load data, preprocess, train, and evaluate.\n",
    "        \"\"\"\n",
    "        self.load_and_combine_data()\n",
    "        self.preprocess_data()\n",
    "        self.train_model()\n",
    "        self.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for compiling metrics of the models\n",
    "\n",
    "svm_models = []\n",
    "mlp_models = []\n",
    "lr_models = []\n",
    "rf_models = []\n",
    "ada_models = []\n",
    "models = []\n",
    "\n",
    "def compile_metrics(models):\n",
    "    metrics_data = [model.metrics for model in models]\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustable hyperparameters and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"AdaBoost\",\n",
    "        \"model_class\": AdaBoostClassifier,\n",
    "        \"params\": {'n_estimators': 50, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVM\",\n",
    "        \"model_class\": SVC,\n",
    "        \"params\": {'C': 1.0, 'kernel': 'linear', 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model_class\": RandomForestClassifier,\n",
    "        \"params\": {'n_estimators': 100, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"model_class\": LogisticRegression,\n",
    "        \"params\": {'max_iter': 500, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLP\",\n",
    "        \"model_class\": MLPClassifier,\n",
    "        \"params\": {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'random_state': 42}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature combinations\n",
    "feature_combinations = [\n",
    "    {\"files\": [empath_file], \"name_suffix\": \"(Empath)\"},\n",
    "    {\"files\": [lda_file], \"name_suffix\": \"(LDA)\"},\n",
    "    {\"files\": [unigram_file], \"name_suffix\": \"(unigram)\"},\n",
    "    {\"files\": [bigram_file], \"name_suffix\": \"(bigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, unigram_file], \"name_suffix\": \"(EM + LDA + unigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, bigram_file], \"name_suffix\": \"(EM + LDA + bigram)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training AdaBoost (Empath)...\n",
      "Loading data for AdaBoost (Empath)...\n",
      "Loaded data shape: (293, 48)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 47), Test set size: (59, 47)\n",
      "Training AdaBoost (Empath)...\n",
      "Model AdaBoost (Empath) trained successfully.\n",
      "Evaluating AdaBoost (Empath)...\n",
      "Accuracy: 0.8983050847457628\n",
      "\n",
      "Training AdaBoost (LDA)...\n",
      "Loading data for AdaBoost (LDA)...\n",
      "Loaded data shape: (293, 31)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 30), Test set size: (59, 30)\n",
      "Training AdaBoost (LDA)...\n",
      "Model AdaBoost (LDA) trained successfully.\n",
      "Evaluating AdaBoost (LDA)...\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Training AdaBoost (unigram)...\n",
      "Loading data for AdaBoost (unigram)...\n",
      "Loaded data shape: (293, 4296)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4295), Test set size: (59, 4295)\n",
      "Training AdaBoost (unigram)...\n",
      "Model AdaBoost (unigram) trained successfully.\n",
      "Evaluating AdaBoost (unigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training AdaBoost (bigram)...\n",
      "Loading data for AdaBoost (bigram)...\n",
      "Loaded data shape: (293, 29709)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29708), Test set size: (59, 29708)\n",
      "Training AdaBoost (bigram)...\n",
      "Model AdaBoost (bigram) trained successfully.\n",
      "Evaluating AdaBoost (bigram)...\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Training AdaBoost (EM + LDA + unigram)...\n",
      "Loading data for AdaBoost (EM + LDA + unigram)...\n",
      "Loaded data shape: (293, 4373)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4372), Test set size: (59, 4372)\n",
      "Training AdaBoost (EM + LDA + unigram)...\n",
      "Model AdaBoost (EM + LDA + unigram) trained successfully.\n",
      "Evaluating AdaBoost (EM + LDA + unigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training AdaBoost (EM + LDA + bigram)...\n",
      "Loading data for AdaBoost (EM + LDA + bigram)...\n",
      "Loaded data shape: (293, 29786)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29785), Test set size: (59, 29785)\n",
      "Training AdaBoost (EM + LDA + bigram)...\n",
      "Model AdaBoost (EM + LDA + bigram) trained successfully.\n",
      "Evaluating AdaBoost (EM + LDA + bigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training SVM (Empath)...\n",
      "Loading data for SVM (Empath)...\n",
      "Loaded data shape: (293, 48)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 47), Test set size: (59, 47)\n",
      "Training SVM (Empath)...\n",
      "Model SVM (Empath) trained successfully.\n",
      "Evaluating SVM (Empath)...\n",
      "Accuracy: 0.8983050847457628\n",
      "\n",
      "Training SVM (LDA)...\n",
      "Loading data for SVM (LDA)...\n",
      "Loaded data shape: (293, 31)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 30), Test set size: (59, 30)\n",
      "Training SVM (LDA)...\n",
      "Model SVM (LDA) trained successfully.\n",
      "Evaluating SVM (LDA)...\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Training SVM (unigram)...\n",
      "Loading data for SVM (unigram)...\n",
      "Loaded data shape: (293, 4296)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4295), Test set size: (59, 4295)\n",
      "Training SVM (unigram)...\n",
      "Model SVM (unigram) trained successfully.\n",
      "Evaluating SVM (unigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training SVM (bigram)...\n",
      "Loading data for SVM (bigram)...\n",
      "Loaded data shape: (293, 29709)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29708), Test set size: (59, 29708)\n",
      "Training SVM (bigram)...\n",
      "Model SVM (bigram) trained successfully.\n",
      "Evaluating SVM (bigram)...\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Training SVM (EM + LDA + unigram)...\n",
      "Loading data for SVM (EM + LDA + unigram)...\n",
      "Loaded data shape: (293, 4373)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4372), Test set size: (59, 4372)\n",
      "Training SVM (EM + LDA + unigram)...\n",
      "Model SVM (EM + LDA + unigram) trained successfully.\n",
      "Evaluating SVM (EM + LDA + unigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training SVM (EM + LDA + bigram)...\n",
      "Loading data for SVM (EM + LDA + bigram)...\n",
      "Loaded data shape: (293, 29786)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29785), Test set size: (59, 29785)\n",
      "Training SVM (EM + LDA + bigram)...\n",
      "Model SVM (EM + LDA + bigram) trained successfully.\n",
      "Evaluating SVM (EM + LDA + bigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training Random Forest (Empath)...\n",
      "Loading data for Random Forest (Empath)...\n",
      "Loaded data shape: (293, 48)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 47), Test set size: (59, 47)\n",
      "Training Random Forest (Empath)...\n",
      "Model Random Forest (Empath) trained successfully.\n",
      "Evaluating Random Forest (Empath)...\n",
      "Accuracy: 0.8813559322033898\n",
      "\n",
      "Training Random Forest (LDA)...\n",
      "Loading data for Random Forest (LDA)...\n",
      "Loaded data shape: (293, 31)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 30), Test set size: (59, 30)\n",
      "Training Random Forest (LDA)...\n",
      "Model Random Forest (LDA) trained successfully.\n",
      "Evaluating Random Forest (LDA)...\n",
      "Accuracy: 0.8813559322033898\n",
      "\n",
      "Training Random Forest (unigram)...\n",
      "Loading data for Random Forest (unigram)...\n",
      "Loaded data shape: (293, 4296)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4295), Test set size: (59, 4295)\n",
      "Training Random Forest (unigram)...\n",
      "Model Random Forest (unigram) trained successfully.\n",
      "Evaluating Random Forest (unigram)...\n",
      "Accuracy: 0.9322033898305084\n",
      "\n",
      "Training Random Forest (bigram)...\n",
      "Loading data for Random Forest (bigram)...\n",
      "Loaded data shape: (293, 29709)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29708), Test set size: (59, 29708)\n",
      "Training Random Forest (bigram)...\n",
      "Model Random Forest (bigram) trained successfully.\n",
      "Evaluating Random Forest (bigram)...\n",
      "Accuracy: 0.7966101694915254\n",
      "\n",
      "Training Random Forest (EM + LDA + unigram)...\n",
      "Loading data for Random Forest (EM + LDA + unigram)...\n",
      "Loaded data shape: (293, 4373)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4372), Test set size: (59, 4372)\n",
      "Training Random Forest (EM + LDA + unigram)...\n",
      "Model Random Forest (EM + LDA + unigram) trained successfully.\n",
      "Evaluating Random Forest (EM + LDA + unigram)...\n",
      "Accuracy: 0.9491525423728814\n",
      "\n",
      "Training Random Forest (EM + LDA + bigram)...\n",
      "Loading data for Random Forest (EM + LDA + bigram)...\n",
      "Loaded data shape: (293, 29786)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29785), Test set size: (59, 29785)\n",
      "Training Random Forest (EM + LDA + bigram)...\n",
      "Model Random Forest (EM + LDA + bigram) trained successfully.\n",
      "Evaluating Random Forest (EM + LDA + bigram)...\n",
      "Accuracy: 0.847457627118644\n",
      "\n",
      "Training Logistic Regression (Empath)...\n",
      "Loading data for Logistic Regression (Empath)...\n",
      "Loaded data shape: (293, 48)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 47), Test set size: (59, 47)\n",
      "Training Logistic Regression (Empath)...\n",
      "Model Logistic Regression (Empath) trained successfully.\n",
      "Evaluating Logistic Regression (Empath)...\n",
      "Accuracy: 0.8983050847457628\n",
      "\n",
      "Training Logistic Regression (LDA)...\n",
      "Loading data for Logistic Regression (LDA)...\n",
      "Loaded data shape: (293, 31)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 30), Test set size: (59, 30)\n",
      "Training Logistic Regression (LDA)...\n",
      "Model Logistic Regression (LDA) trained successfully.\n",
      "Evaluating Logistic Regression (LDA)...\n",
      "Accuracy: 0.8135593220338984\n",
      "\n",
      "Training Logistic Regression (unigram)...\n",
      "Loading data for Logistic Regression (unigram)...\n",
      "Loaded data shape: (293, 4296)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4295), Test set size: (59, 4295)\n",
      "Training Logistic Regression (unigram)...\n",
      "Model Logistic Regression (unigram) trained successfully.\n",
      "Evaluating Logistic Regression (unigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training Logistic Regression (bigram)...\n",
      "Loading data for Logistic Regression (bigram)...\n",
      "Loaded data shape: (293, 29709)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29708), Test set size: (59, 29708)\n",
      "Training Logistic Regression (bigram)...\n",
      "Model Logistic Regression (bigram) trained successfully.\n",
      "Evaluating Logistic Regression (bigram)...\n",
      "Accuracy: 0.7627118644067796\n",
      "\n",
      "Training Logistic Regression (EM + LDA + unigram)...\n",
      "Loading data for Logistic Regression (EM + LDA + unigram)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data shape: (293, 4373)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4372), Test set size: (59, 4372)\n",
      "Training Logistic Regression (EM + LDA + unigram)...\n",
      "Model Logistic Regression (EM + LDA + unigram) trained successfully.\n",
      "Evaluating Logistic Regression (EM + LDA + unigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training Logistic Regression (EM + LDA + bigram)...\n",
      "Loading data for Logistic Regression (EM + LDA + bigram)...\n",
      "Loaded data shape: (293, 29786)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29785), Test set size: (59, 29785)\n",
      "Training Logistic Regression (EM + LDA + bigram)...\n",
      "Model Logistic Regression (EM + LDA + bigram) trained successfully.\n",
      "Evaluating Logistic Regression (EM + LDA + bigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training MLP (Empath)...\n",
      "Loading data for MLP (Empath)...\n",
      "Loaded data shape: (293, 48)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 47), Test set size: (59, 47)\n",
      "Training MLP (Empath)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MLP (Empath) trained successfully.\n",
      "Evaluating MLP (Empath)...\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Training MLP (LDA)...\n",
      "Loading data for MLP (LDA)...\n",
      "Loaded data shape: (293, 31)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 30), Test set size: (59, 30)\n",
      "Training MLP (LDA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/aai/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MLP (LDA) trained successfully.\n",
      "Evaluating MLP (LDA)...\n",
      "Accuracy: 0.8305084745762712\n",
      "\n",
      "Training MLP (unigram)...\n",
      "Loading data for MLP (unigram)...\n",
      "Loaded data shape: (293, 4296)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4295), Test set size: (59, 4295)\n",
      "Training MLP (unigram)...\n",
      "Model MLP (unigram) trained successfully.\n",
      "Evaluating MLP (unigram)...\n",
      "Accuracy: 1.0\n",
      "\n",
      "Training MLP (bigram)...\n",
      "Loading data for MLP (bigram)...\n",
      "Loaded data shape: (293, 29709)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29708), Test set size: (59, 29708)\n",
      "Training MLP (bigram)...\n",
      "Model MLP (bigram) trained successfully.\n",
      "Evaluating MLP (bigram)...\n",
      "Accuracy: 0.864406779661017\n",
      "\n",
      "Training MLP (EM + LDA + unigram)...\n",
      "Loading data for MLP (EM + LDA + unigram)...\n",
      "Loaded data shape: (293, 4373)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 4372), Test set size: (59, 4372)\n",
      "Training MLP (EM + LDA + unigram)...\n",
      "Model MLP (EM + LDA + unigram) trained successfully.\n",
      "Evaluating MLP (EM + LDA + unigram)...\n",
      "Accuracy: 0.9322033898305084\n",
      "\n",
      "Training MLP (EM + LDA + bigram)...\n",
      "Loading data for MLP (EM + LDA + bigram)...\n",
      "Loaded data shape: (293, 29786)\n",
      "Splitting data into train and test sets...\n",
      "Training set size: (234, 29785), Test set size: (59, 29785)\n",
      "Training MLP (EM + LDA + bigram)...\n",
      "Model MLP (EM + LDA + bigram) trained successfully.\n",
      "Evaluating MLP (EM + LDA + bigram)...\n",
      "Accuracy: 0.9152542372881356\n",
      "\n",
      "Overall Metrics Table:\n",
      "                                       Model  Accuracy  F1 Score  Precision  \\\n",
      "0                          AdaBoost (Empath)  0.898305  0.898305   0.898305   \n",
      "1                             AdaBoost (LDA)  0.864407  0.864407   0.864407   \n",
      "2                         AdaBoost (unigram)  1.000000  1.000000   1.000000   \n",
      "3                          AdaBoost (bigram)  0.847458  0.818263   0.872881   \n",
      "4              AdaBoost (EM + LDA + unigram)  1.000000  1.000000   1.000000   \n",
      "5               AdaBoost (EM + LDA + bigram)  1.000000  1.000000   1.000000   \n",
      "6                               SVM (Empath)  0.898305  0.898305   0.898305   \n",
      "7                                  SVM (LDA)  0.813559  0.789213   0.800196   \n",
      "8                              SVM (unigram)  1.000000  1.000000   1.000000   \n",
      "9                               SVM (bigram)  0.796610  0.732303   0.839429   \n",
      "10                  SVM (EM + LDA + unigram)  1.000000  1.000000   1.000000   \n",
      "11                   SVM (EM + LDA + bigram)  1.000000  1.000000   1.000000   \n",
      "12                    Random Forest (Empath)  0.881356  0.871582   0.882109   \n",
      "13                       Random Forest (LDA)  0.881356  0.876151   0.877408   \n",
      "14                   Random Forest (unigram)  0.932203  0.927996   0.937738   \n",
      "15                    Random Forest (bigram)  0.796610  0.732303   0.839429   \n",
      "16        Random Forest (EM + LDA + unigram)  0.949153  0.946922   0.952331   \n",
      "17         Random Forest (EM + LDA + bigram)  0.847458  0.818263   0.872881   \n",
      "18              Logistic Regression (Empath)  0.898305  0.895499   0.895540   \n",
      "19                 Logistic Regression (LDA)  0.813559  0.789213   0.800196   \n",
      "20             Logistic Regression (unigram)  1.000000  1.000000   1.000000   \n",
      "21              Logistic Regression (bigram)  0.762712  0.660039   0.581729   \n",
      "22  Logistic Regression (EM + LDA + unigram)  1.000000  1.000000   1.000000   \n",
      "23   Logistic Regression (EM + LDA + bigram)  1.000000  1.000000   1.000000   \n",
      "24                              MLP (Empath)  0.864407  0.860665   0.859538   \n",
      "25                                 MLP (LDA)  0.830508  0.819990   0.819855   \n",
      "26                             MLP (unigram)  1.000000  1.000000   1.000000   \n",
      "27                              MLP (bigram)  0.864407  0.842823   0.884874   \n",
      "28                  MLP (EM + LDA + unigram)  0.932203  0.927996   0.937738   \n",
      "29                   MLP (EM + LDA + bigram)  0.915254  0.911536   0.914869   \n",
      "\n",
      "      Recall  \n",
      "0   0.898305  \n",
      "1   0.864407  \n",
      "2   1.000000  \n",
      "3   0.847458  \n",
      "4   1.000000  \n",
      "5   1.000000  \n",
      "6   0.898305  \n",
      "7   0.813559  \n",
      "8   1.000000  \n",
      "9   0.796610  \n",
      "10  1.000000  \n",
      "11  1.000000  \n",
      "12  0.881356  \n",
      "13  0.881356  \n",
      "14  0.932203  \n",
      "15  0.796610  \n",
      "16  0.949153  \n",
      "17  0.847458  \n",
      "18  0.898305  \n",
      "19  0.813559  \n",
      "20  1.000000  \n",
      "21  0.762712  \n",
      "22  1.000000  \n",
      "23  1.000000  \n",
      "24  0.864407  \n",
      "25  0.830508  \n",
      "26  1.000000  \n",
      "27  0.864407  \n",
      "28  0.932203  \n",
      "29  0.915254  \n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "all_models = []\n",
    "model_specific_results = {}\n",
    "\n",
    "\n",
    "for model_config in models_config:\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_class = model_config[\"model_class\"]\n",
    "    model_params = model_config[\"params\"]\n",
    "    \n",
    "    # For each feature combination\n",
    "    specific_models = []\n",
    "    for feature_combo in feature_combinations:\n",
    "        feature_files = feature_combo[\"files\"]\n",
    "        name_suffix = feature_combo[\"name_suffix\"]\n",
    "        full_model_name = f\"{model_name} {name_suffix}\"\n",
    "        \n",
    "        print(f\"\\nTraining {full_model_name}...\")\n",
    "        trainer = ModelTrainer(feature_files, model_class, full_model_name, model_params)\n",
    "        trainer.run_pipeline()\n",
    "        \n",
    "        # Store the model and its results\n",
    "        all_models.append(trainer)\n",
    "        specific_models.append(trainer)\n",
    "    \n",
    "    # Compile metrics for the specific model type\n",
    "    model_specific_results[model_name] = compile_metrics(specific_models)\n",
    "\n",
    "# Compile overall metrics for all models\n",
    "overall_metrics_table = compile_metrics(all_models)\n",
    "print(\"\\nOverall Metrics Table:\")\n",
    "print(overall_metrics_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
