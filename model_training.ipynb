{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\pandas\\core\\arrays\\masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "# Installing dependencies \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetching the preprocessed data as 'posts' and 'labels' to be used also\n",
    "\n",
    "models = []\n",
    "posts = []  \n",
    "labels = [] \n",
    "\n",
    "folders = {\n",
    "    \"depression\": {\n",
    "        \"path\": \"data/preprocessed_posts/depression\",\n",
    "        \"label\": 1  # Label for depression-related posts\n",
    "    },\n",
    "    \"breastcancer\": {\n",
    "        \"path\": \"data/preprocessed_posts/standard\",\n",
    "        \"label\": 0  # Label for breast cancer posts\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, data in folders.items():\n",
    "    folder_path = data[\"path\"]\n",
    "    label = data[\"label\"]\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()  # Read the file content\n",
    "                posts.append(content)  # Add to postst list\n",
    "                labels.append(label)  # Add corresponding label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the feature extraction data\n",
    "empath_file = \"data/feature_extracted_data/empath_features_with_labels.csv\"\n",
    "lda_file = \"data/feature_extracted_data/lda_topic_distributions_with_labels.csv\"\n",
    "unigram_file = \"data/feature_extracted_data/unigram_features_with_labels.csv\"\n",
    "bigram_file = \"data/feature_extracted_data/bigram_features_with_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ModelTrainer class\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, csv_files, model, model_name, model_params=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the ModelTrainer class.\n",
    "\n",
    "        Parameters:\n",
    "        csv_files (list of str): List of file paths for the feature datasets (CSV files).\n",
    "        model (class): Machine learning model class (e.g., LogisticRegression, SVC).\n",
    "        model_name (str): Name of the model for identification.\n",
    "        model_params (dict): Parameters for the model.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model\n",
    "        self.model_params = model_params if model_params else {}\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.random_state = random_state\n",
    "        self.metrics = {}\n",
    "\n",
    "    def load_and_combine_data(self):\n",
    "        \"\"\"\n",
    "        Load and combine data from multiple CSV files into a single dataset.\n",
    "        Assumes each CSV has the same 'label' column.\n",
    "        \"\"\"\n",
    "        data_frames = [pd.read_csv(file) for file in self.csv_files]\n",
    "\n",
    "        # Ensure labels are consistent and take from the first dataset\n",
    "        labels = data_frames[0]['label']\n",
    "        for df in data_frames[1:]:\n",
    "            if 'label' in df.columns:\n",
    "                df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "        combined_data = pd.concat(data_frames, axis=1)\n",
    "\n",
    "        # Check alignment between features and labels\n",
    "        if len(labels) != len(combined_data):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between features and labels: \"\n",
    "                f\"{len(combined_data)} rows in features, {len(labels)} in labels.\"\n",
    "            )\n",
    "\n",
    "        # Add the label column\n",
    "        self.data = combined_data\n",
    "        self.data['label'] = labels\n",
    "\n",
    "    def preprocess_data(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "        test_size (float): Proportion of data to use for testing.\n",
    "        \"\"\"\n",
    "        X = self.data.iloc[:, :-1]  # All columns except the label column\n",
    "        y = self.data['label']  # Label column\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state\n",
    "        )\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train the specified model on the training data.\n",
    "        \"\"\"\n",
    "        self.model = self.model_class(**self.model_params)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(f\"Model {self.model_name} trained successfully.\")\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test data and print metrics, including 10-fold cross-validation.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(f\"Model {self.model_name} has not been trained yet.\")\n",
    "\n",
    "        # Predictions and metrics for the test set\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        precision = precision_score(self.y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(self.y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        cv_scores = cross_val_score(self.model, self.X_train, self.y_train, cv=10, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "\n",
    "\n",
    "        # Store metrics\n",
    "        self.metrics = {\n",
    "            \"Model\": self.model_name,\n",
    "            \"Test Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"CV Mean Accuracy\": cv_mean,\n",
    "            \"CV Std Dev\": cv_std\n",
    "        }\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Complete pipeline: load data, preprocess, train, and evaluate.\n",
    "        \"\"\"\n",
    "        self.load_and_combine_data()\n",
    "        self.preprocess_data()\n",
    "        self.train_model()\n",
    "        self.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for compiling metrics of the models\n",
    "\n",
    "svm_models = []\n",
    "mlp_models = []\n",
    "lr_models = []\n",
    "rf_models = []\n",
    "ada_models = []\n",
    "models = []\n",
    "\n",
    "def compile_metrics(models):\n",
    "    metrics_data = [model.metrics for model in models]\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustable hyperparameters and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"AdaBoost\",\n",
    "        \"model_class\": AdaBoostClassifier,\n",
    "        \"params\": {'n_estimators': 50, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVM\",\n",
    "        \"model_class\": SVC,\n",
    "        \"params\": {'C': 1.0, 'kernel': 'linear', 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model_class\": RandomForestClassifier,\n",
    "        \"params\": {'n_estimators': 100, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"model_class\": LogisticRegression,\n",
    "        \"params\": {'max_iter': 500, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLP\",\n",
    "        \"model_class\": MLPClassifier,\n",
    "        \"params\": {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'random_state': 42}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature combinations\n",
    "feature_combinations = [\n",
    "    {\"files\": [empath_file], \"name_suffix\": \"(Empath)\"},\n",
    "    {\"files\": [lda_file], \"name_suffix\": \"(LDA)\"},\n",
    "    {\"files\": [unigram_file], \"name_suffix\": \"(unigram)\"},\n",
    "    {\"files\": [bigram_file], \"name_suffix\": \"(bigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, unigram_file], \"name_suffix\": \"(EM + LDA + unigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, bigram_file], \"name_suffix\": \"(EM + LDA + bigram)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training AdaBoost (Empath)...\n",
      "Model AdaBoost (Empath) trained successfully.\n",
      "\n",
      "Training AdaBoost (LDA)...\n",
      "Model AdaBoost (LDA) trained successfully.\n",
      "\n",
      "Training AdaBoost (unigram)...\n",
      "Model AdaBoost (unigram) trained successfully.\n",
      "\n",
      "Training AdaBoost (bigram)...\n",
      "Model AdaBoost (bigram) trained successfully.\n",
      "\n",
      "Training AdaBoost (EM + LDA + unigram)...\n",
      "Model AdaBoost (EM + LDA + unigram) trained successfully.\n",
      "\n",
      "Training AdaBoost (EM + LDA + bigram)...\n",
      "Model AdaBoost (EM + LDA + bigram) trained successfully.\n",
      "\n",
      "Training SVM (Empath)...\n",
      "Model SVM (Empath) trained successfully.\n",
      "\n",
      "Training SVM (LDA)...\n",
      "Model SVM (LDA) trained successfully.\n",
      "\n",
      "Training SVM (unigram)...\n",
      "Model SVM (unigram) trained successfully.\n",
      "\n",
      "Training SVM (bigram)...\n",
      "Model SVM (bigram) trained successfully.\n",
      "\n",
      "Training SVM (EM + LDA + unigram)...\n",
      "Model SVM (EM + LDA + unigram) trained successfully.\n",
      "\n",
      "Training SVM (EM + LDA + bigram)...\n",
      "Model SVM (EM + LDA + bigram) trained successfully.\n",
      "\n",
      "Training Random Forest (Empath)...\n",
      "Model Random Forest (Empath) trained successfully.\n",
      "\n",
      "Training Random Forest (LDA)...\n",
      "Model Random Forest (LDA) trained successfully.\n",
      "\n",
      "Training Random Forest (unigram)...\n",
      "Model Random Forest (unigram) trained successfully.\n",
      "\n",
      "Training Random Forest (bigram)...\n",
      "Model Random Forest (bigram) trained successfully.\n",
      "\n",
      "Training Random Forest (EM + LDA + unigram)...\n",
      "Model Random Forest (EM + LDA + unigram) trained successfully.\n",
      "\n",
      "Training Random Forest (EM + LDA + bigram)...\n",
      "Model Random Forest (EM + LDA + bigram) trained successfully.\n",
      "\n",
      "Training Logistic Regression (Empath)...\n",
      "Model Logistic Regression (Empath) trained successfully.\n",
      "\n",
      "Training Logistic Regression (LDA)...\n",
      "Model Logistic Regression (LDA) trained successfully.\n",
      "\n",
      "Training Logistic Regression (unigram)...\n",
      "Model Logistic Regression (unigram) trained successfully.\n",
      "\n",
      "Training Logistic Regression (bigram)...\n",
      "Model Logistic Regression (bigram) trained successfully.\n",
      "\n",
      "Training Logistic Regression (EM + LDA + unigram)...\n",
      "Model Logistic Regression (EM + LDA + unigram) trained successfully.\n",
      "\n",
      "Training Logistic Regression (EM + LDA + bigram)...\n",
      "Model Logistic Regression (EM + LDA + bigram) trained successfully.\n",
      "\n",
      "Training MLP (Empath)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MLP (Empath) trained successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP (LDA)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model MLP (LDA) trained successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n",
      "c:\\Users\\jgber\\anaconda3\\lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:684: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training MLP (unigram)...\n",
      "Model MLP (unigram) trained successfully.\n",
      "\n",
      "Training MLP (bigram)...\n",
      "Model MLP (bigram) trained successfully.\n",
      "\n",
      "Training MLP (EM + LDA + unigram)...\n",
      "Model MLP (EM + LDA + unigram) trained successfully.\n",
      "\n",
      "Training MLP (EM + LDA + bigram)...\n",
      "Model MLP (EM + LDA + bigram) trained successfully.\n",
      "\n",
      "Overall Metrics Table:\n",
      "                                       Model  Test Accuracy  F1 Score  \\\n",
      "0                          AdaBoost (Empath)       0.915344  0.915042   \n",
      "1                             AdaBoost (LDA)       0.915344  0.914761   \n",
      "2                         AdaBoost (unigram)       1.000000  1.000000   \n",
      "3                          AdaBoost (bigram)       0.957672  0.957823   \n",
      "4              AdaBoost (EM + LDA + unigram)       1.000000  1.000000   \n",
      "5               AdaBoost (EM + LDA + bigram)       1.000000  1.000000   \n",
      "6                               SVM (Empath)       0.912698  0.912514   \n",
      "7                                  SVM (LDA)       0.920635  0.920694   \n",
      "8                              SVM (unigram)       1.000000  1.000000   \n",
      "9                               SVM (bigram)       0.960317  0.960482   \n",
      "10                  SVM (EM + LDA + unigram)       1.000000  1.000000   \n",
      "11                   SVM (EM + LDA + bigram)       1.000000  1.000000   \n",
      "12                    Random Forest (Empath)       0.933862  0.933781   \n",
      "13                       Random Forest (LDA)       0.936508  0.936508   \n",
      "14                   Random Forest (unigram)       0.994709  0.994713   \n",
      "15                    Random Forest (bigram)       0.965608  0.965738   \n",
      "16        Random Forest (EM + LDA + unigram)       0.968254  0.968367   \n",
      "17         Random Forest (EM + LDA + bigram)       0.949735  0.949823   \n",
      "18              Logistic Regression (Empath)       0.912698  0.912342   \n",
      "19                 Logistic Regression (LDA)       0.920635  0.920503   \n",
      "20             Logistic Regression (unigram)       1.000000  1.000000   \n",
      "21              Logistic Regression (bigram)       0.962963  0.963110   \n",
      "22  Logistic Regression (EM + LDA + unigram)       1.000000  1.000000   \n",
      "23   Logistic Regression (EM + LDA + bigram)       1.000000  1.000000   \n",
      "24                              MLP (Empath)       0.917989  0.917738   \n",
      "25                                 MLP (LDA)       0.925926  0.926031   \n",
      "26                             MLP (unigram)       0.986772  0.986796   \n",
      "27                              MLP (bigram)       0.960317  0.960482   \n",
      "28                  MLP (EM + LDA + unigram)       0.986772  0.986787   \n",
      "29                   MLP (EM + LDA + bigram)       0.984127  0.984127   \n",
      "\n",
      "    Precision    Recall  CV Mean Accuracy  CV Std Dev  \n",
      "0    0.915584  0.915344          0.939818    0.023207  \n",
      "1    0.916795  0.915344          0.929880    0.022019  \n",
      "2    1.000000  1.000000          1.000000    0.000000  \n",
      "3    0.959915  0.957672          0.959001    0.016913  \n",
      "4    1.000000  1.000000          1.000000    0.000000  \n",
      "5    1.000000  1.000000          1.000000    0.000000  \n",
      "6    0.912656  0.912698          0.937844    0.015094  \n",
      "7    0.920805  0.920635          0.926608    0.024380  \n",
      "8    1.000000  1.000000          1.000000    0.000000  \n",
      "9    0.963680  0.960317          0.955694    0.015641  \n",
      "10   1.000000  1.000000          1.000000    0.000000  \n",
      "11   1.000000  1.000000          1.000000    0.000000  \n",
      "12   0.933821  0.933862          0.944441    0.020115  \n",
      "13   0.936508  0.936508          0.943129    0.018715  \n",
      "14   0.994774  0.994709          0.988755    0.009845  \n",
      "15   0.968163  0.965608          0.949072    0.028095  \n",
      "16   0.970443  0.968254          0.961646    0.015002  \n",
      "17   0.950247  0.949735          0.959001    0.016403  \n",
      "18   0.913053  0.912698          0.922639    0.020438  \n",
      "19   0.920582  0.920635          0.920652    0.021850  \n",
      "20   1.000000  1.000000          1.000000    0.000000  \n",
      "21   0.965909  0.962963          0.955694    0.015641  \n",
      "22   1.000000  1.000000          0.998675    0.002649  \n",
      "23   1.000000  1.000000          0.997355    0.003239  \n",
      "24   0.918134  0.917989          0.939164    0.018406  \n",
      "25   0.926347  0.925926          0.929902    0.023657  \n",
      "26   0.987169  0.986772          0.986781    0.011045  \n",
      "27   0.963680  0.960317          0.956357    0.014519  \n",
      "28   0.986928  0.986772          0.979501    0.013360  \n",
      "29   0.984127  0.984127          0.981492    0.010141  \n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "all_models = []\n",
    "model_specific_results = {}\n",
    "\n",
    "\n",
    "for model_config in models_config:\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_class = model_config[\"model_class\"]\n",
    "    model_params = model_config[\"params\"]\n",
    "    \n",
    "    # For each feature combination\n",
    "    specific_models = []\n",
    "    for feature_combo in feature_combinations:\n",
    "        feature_files = feature_combo[\"files\"]\n",
    "        name_suffix = feature_combo[\"name_suffix\"]\n",
    "        full_model_name = f\"{model_name} {name_suffix}\"\n",
    "        \n",
    "        print(f\"\\nTraining {full_model_name}...\")\n",
    "        trainer = ModelTrainer(feature_files, model_class, full_model_name, model_params)\n",
    "        trainer.run_pipeline()\n",
    "        \n",
    "        # Store the model and its results\n",
    "        all_models.append(trainer)\n",
    "        specific_models.append(trainer)\n",
    "    \n",
    "    # Compile metrics for the specific model type\n",
    "    model_specific_results[model_name] = compile_metrics(specific_models)\n",
    "\n",
    "# Compile overall metrics for all models\n",
    "overall_metrics_table = compile_metrics(all_models)\n",
    "print(\"\\nOverall Metrics Table:\")\n",
    "print(overall_metrics_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
