{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisite Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing dependencies \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "import os\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/preprocessed/preprocessed_depression_posts'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpath\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     20\u001b[0m label \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_name \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfolder_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m     22\u001b[0m     file_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, file_name)\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(file_path):\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/preprocessed/preprocessed_depression_posts'"
     ]
    }
   ],
   "source": [
    "# Fetching the preprocessed data as 'posts' and 'labels' to be used also\n",
    "\n",
    "models = []\n",
    "posts = []  \n",
    "labels = [] \n",
    "\n",
    "folders = {\n",
    "    \"depression\": {\n",
    "        \"path\": \"data/preprocessed/preprocessed_depression_posts\",\n",
    "        \"label\": 1  # Label for depression-related posts\n",
    "    },\n",
    "    \"breastcancer\": {\n",
    "        \"path\": \"data/preprocessed/preprocessed_breastcancer_posts\",\n",
    "        \"label\": 0  # Label for breast cancer posts\n",
    "    }\n",
    "}\n",
    "\n",
    "for category, data in folders.items():\n",
    "    folder_path = data[\"path\"]\n",
    "    label = data[\"label\"]\n",
    "    for file_name in os.listdir(folder_path):\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        if os.path.isfile(file_path):\n",
    "            with open(file_path, 'r', encoding='utf-8') as file:\n",
    "                content = file.read()  # Read the file content\n",
    "                posts.append(content)  # Add to postst list\n",
    "                labels.append(label)  # Add corresponding label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the feature extraction data\n",
    "empath_file = \"data/feature_extracted_data/empath_features_with_labels.csv\"\n",
    "lda_file = \"data/feature_extracted_data/lda_topic_distributions_with_labels.csv\"\n",
    "unigram_file = \"data/feature_extracted_data/unigram_features_with_labels.csv\"\n",
    "bigram_file = \"data/feature_extracted_data/bigram_features_with_labels.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a ModelTrainer class\n",
    "\n",
    "class ModelTrainer:\n",
    "    def __init__(self, csv_files, model, model_name, model_params=None, random_state=42):\n",
    "        \"\"\"\n",
    "        Initialize the ModelTrainer class.\n",
    "\n",
    "        Parameters:\n",
    "        csv_files (list of str): List of file paths for the feature datasets (CSV files).\n",
    "        model (class): Machine learning model class (e.g., LogisticRegression, SVC).\n",
    "        model_name (str): Name of the model for identification.\n",
    "        model_params (dict): Parameters for the model.\n",
    "        random_state (int): Random seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.csv_files = csv_files\n",
    "        self.model_name = model_name\n",
    "        self.model_class = model\n",
    "        self.model_params = model_params if model_params else {}\n",
    "        self.data = None\n",
    "        self.model = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.random_state = random_state\n",
    "        self.metrics = {}\n",
    "\n",
    "    def load_and_combine_data(self):\n",
    "        \"\"\"\n",
    "        Load and combine data from multiple CSV files into a single dataset.\n",
    "        Assumes each CSV has the same 'label' column.\n",
    "        \"\"\"\n",
    "        print(f\"Loading data for {self.model_name}...\")\n",
    "        data_frames = [pd.read_csv(file) for file in self.csv_files]\n",
    "\n",
    "        # Ensure labels are consistent and take from the first dataset\n",
    "        labels = data_frames[0]['label']\n",
    "        for df in data_frames[1:]:\n",
    "            if 'label' in df.columns:\n",
    "                df.drop(columns=['label'], inplace=True)\n",
    "\n",
    "        combined_data = pd.concat(data_frames, axis=1)\n",
    "\n",
    "        # Check alignment between features and labels\n",
    "        if len(labels) != len(combined_data):\n",
    "            raise ValueError(\n",
    "                f\"Mismatch between features and labels: \"\n",
    "                f\"{len(combined_data)} rows in features, {len(labels)} in labels.\"\n",
    "            )\n",
    "\n",
    "        # Add the label column\n",
    "        self.data = combined_data\n",
    "        self.data['label'] = labels\n",
    "        print(f\"Loaded data shape: {self.data.shape}\")\n",
    "\n",
    "    def preprocess_data(self, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Split the data into training and testing sets.\n",
    "\n",
    "        Parameters:\n",
    "        test_size (float): Proportion of data to use for testing.\n",
    "        \"\"\"\n",
    "        print(\"Splitting data into train and test sets...\")\n",
    "        X = self.data.iloc[:, :-1]  # All columns except the label column\n",
    "        y = self.data['label']  # Label column\n",
    "\n",
    "        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "            X, y, test_size=test_size, random_state=self.random_state\n",
    "        )\n",
    "        print(f\"Training set size: {self.X_train.shape}, Test set size: {self.X_test.shape}\")\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train the specified model on the training data.\n",
    "        \"\"\"\n",
    "        print(f\"Training {self.model_name}...\")\n",
    "        self.model = self.model_class(**self.model_params)\n",
    "        self.model.fit(self.X_train, self.y_train)\n",
    "        print(f\"Model {self.model_name} trained successfully.\")\n",
    "\n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on the test data and print metrics, including 10-fold cross-validation.\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            raise ValueError(f\"Model {self.model_name} has not been trained yet.\")\n",
    "\n",
    "        print(f\"Evaluating {self.model_name}...\")\n",
    "\n",
    "        # Predictions and metrics for the test set\n",
    "        y_pred = self.model.predict(self.X_test)\n",
    "        accuracy = accuracy_score(self.y_test, y_pred)\n",
    "        f1 = f1_score(self.y_test, y_pred, average='weighted')\n",
    "        precision = precision_score(self.y_test, y_pred, average='weighted')\n",
    "        recall = recall_score(self.y_test, y_pred, average='weighted')\n",
    "\n",
    "        # Perform 10-fold cross-validation\n",
    "        cv_scores = cross_val_score(self.model, self.X_train, self.y_train, cv=10, scoring='accuracy')\n",
    "        cv_mean = cv_scores.mean()\n",
    "        cv_std = cv_scores.std()\n",
    "\n",
    "        print(\"\\nMetrics on Test Data:\")\n",
    "        print(f\"Accuracy: {accuracy:.2f}\")\n",
    "        print(f\"F1 Score: {f1:.2f}\")\n",
    "        print(f\"Precision: {precision:.2f}\")\n",
    "        print(f\"Recall: {recall:.2f}\")\n",
    "\n",
    "        print(\"\\n10-Fold Cross-Validation Results:\")\n",
    "        print(f\"Mean Accuracy: {cv_mean:.2f}\")\n",
    "        print(f\"Standard Deviation: {cv_std:.2f}\")\n",
    "\n",
    "        # Store metrics\n",
    "        self.metrics = {\n",
    "            \"Model\": self.model_name,\n",
    "            \"Test Accuracy\": accuracy,\n",
    "            \"F1 Score\": f1,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"CV Mean Accuracy\": cv_mean,\n",
    "            \"CV Std Dev\": cv_std\n",
    "        }\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"\n",
    "        Complete pipeline: load data, preprocess, train, and evaluate.\n",
    "        \"\"\"\n",
    "        self.load_and_combine_data()\n",
    "        self.preprocess_data()\n",
    "        self.train_model()\n",
    "        self.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a function for compiling metrics of the models\n",
    "\n",
    "svm_models = []\n",
    "mlp_models = []\n",
    "lr_models = []\n",
    "rf_models = []\n",
    "ada_models = []\n",
    "models = []\n",
    "\n",
    "def compile_metrics(models):\n",
    "    metrics_data = [model.metrics for model in models]\n",
    "    metrics_df = pd.DataFrame(metrics_data)\n",
    "    return metrics_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjustable hyperparameters and inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configurations\n",
    "models_config = [\n",
    "    {\n",
    "        \"name\": \"AdaBoost\",\n",
    "        \"model_class\": AdaBoostClassifier,\n",
    "        \"params\": {'n_estimators': 50, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"SVM\",\n",
    "        \"model_class\": SVC,\n",
    "        \"params\": {'C': 1.0, 'kernel': 'linear', 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Random Forest\",\n",
    "        \"model_class\": RandomForestClassifier,\n",
    "        \"params\": {'n_estimators': 100, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Logistic Regression\",\n",
    "        \"model_class\": LogisticRegression,\n",
    "        \"params\": {'max_iter': 500, 'random_state': 42}\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"MLP\",\n",
    "        \"model_class\": MLPClassifier,\n",
    "        \"params\": {'hidden_layer_sizes': (100,), 'activation': 'relu', 'solver': 'adam', 'random_state': 42}\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'empath_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Define feature combinations\u001b[39;00m\n\u001b[1;32m      2\u001b[0m feature_combinations \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m----> 3\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[43mempath_file\u001b[49m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(Empath)\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      4\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: [lda_file], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(LDA)\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      5\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: [unigram_file], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(unigram)\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      6\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: [bigram_file], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(bigram)\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      7\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: [empath_file, lda_file, unigram_file], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(EM + LDA + unigram)\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m: [empath_file, lda_file, bigram_file], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(EM + LDA + bigram)\u001b[39m\u001b[38;5;124m\"\u001b[39m}\n\u001b[1;32m      9\u001b[0m ]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'empath_file' is not defined"
     ]
    }
   ],
   "source": [
    "# Define feature combinations\n",
    "feature_combinations = [\n",
    "    {\"files\": [empath_file], \"name_suffix\": \"(Empath)\"},\n",
    "    {\"files\": [lda_file], \"name_suffix\": \"(LDA)\"},\n",
    "    {\"files\": [unigram_file], \"name_suffix\": \"(unigram)\"},\n",
    "    {\"files\": [bigram_file], \"name_suffix\": \"(bigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, unigram_file], \"name_suffix\": \"(EM + LDA + unigram)\"},\n",
    "    {\"files\": [empath_file, lda_file, bigram_file], \"name_suffix\": \"(EM + LDA + bigram)\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'feature_combinations' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# For each feature combination\u001b[39;00m\n\u001b[1;32m     12\u001b[0m specific_models \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_combo \u001b[38;5;129;01min\u001b[39;00m \u001b[43mfeature_combinations\u001b[49m:\n\u001b[1;32m     14\u001b[0m     feature_files \u001b[38;5;241m=\u001b[39m feature_combo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfiles\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     15\u001b[0m     name_suffix \u001b[38;5;241m=\u001b[39m feature_combo[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mname_suffix\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'feature_combinations' is not defined"
     ]
    }
   ],
   "source": [
    "# Train and evaluate models\n",
    "all_models = []\n",
    "model_specific_results = {}\n",
    "\n",
    "\n",
    "for model_config in models_config:\n",
    "    model_name = model_config[\"name\"]\n",
    "    model_class = model_config[\"model_class\"]\n",
    "    model_params = model_config[\"params\"]\n",
    "    \n",
    "    # For each feature combination\n",
    "    specific_models = []\n",
    "    for feature_combo in feature_combinations:\n",
    "        feature_files = feature_combo[\"files\"]\n",
    "        name_suffix = feature_combo[\"name_suffix\"]\n",
    "        full_model_name = f\"{model_name} {name_suffix}\"\n",
    "        \n",
    "        print(f\"\\nTraining {full_model_name}...\")\n",
    "        trainer = ModelTrainer(feature_files, model_class, full_model_name, model_params)\n",
    "        trainer.run_pipeline()\n",
    "        \n",
    "        # Store the model and its results\n",
    "        all_models.append(trainer)\n",
    "        specific_models.append(trainer)\n",
    "    \n",
    "    # Compile metrics for the specific model type\n",
    "    model_specific_results[model_name] = compile_metrics(specific_models)\n",
    "\n",
    "# Compile overall metrics for all models\n",
    "overall_metrics_table = compile_metrics(all_models)\n",
    "print(\"\\nOverall Metrics Table:\")\n",
    "print(overall_metrics_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
