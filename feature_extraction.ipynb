{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunflowerseed17/PythonDSProject/blob/main/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/nataszasiwinska/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/nataszasiwinska/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from empath import Empath\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base Class\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, documents, labels, output_folder=\"data/feature_extracted_data\"):\n",
        "        self.documents = documents\n",
        "        self.labels = labels\n",
        "        self.output_folder = output_folder\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize, lowercase, remove stopwords, and stem.\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        return [stemmer.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    def save_to_csv(self, data, filename):\n",
        "        \"\"\"\n",
        "        Save data to a CSV file.\n",
        "        \"\"\"\n",
        "        # Construct the full file path\n",
        "        if not filename.startswith(self.output_folder):\n",
        "            file_path = os.path.join(self.output_folder, filename)\n",
        "        else:\n",
        "            file_path = filename\n",
        "\n",
        "        # Debugging: Print the path being used\n",
        "        print(f\"Saving file to: {file_path}\")\n",
        "\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        # Save the file if it doesn't already exist\n",
        "        if not os.path.exists(file_path):\n",
        "            data.to_csv(file_path, index=False)\n",
        "            print(f\"Saved to {file_path}.\")\n",
        "        else:\n",
        "            print(f\"File already exists at {file_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# N-Gram Feature Extractor\n",
        "class NGramFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, documents, labels, output_folder=\"data/feature_extracted_data\"):\n",
        "        super().__init__(documents, labels, output_folder)\n",
        "        self.vectorizer_unigram = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
        "        self.vectorizer_bigram = TfidfVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "        self.vectorizer_combined = TfidfVectorizer(ngram_range=(1, 2), stop_words='english') \n",
        "        self.unigram_matrix = None\n",
        "        self.bigram_matrix = None\n",
        "        self.unigram_feature_names = None\n",
        "        self.bigram_feature_names = None\n",
        "        self.combined_matrix = None\n",
        "        self.combined_feature_names = None\n",
        "\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "\n",
        "    def extract_features(self):\n",
        "        \"\"\"\n",
        "        Extract unigram and bigram features using TF-IDF.\n",
        "        \"\"\"\n",
        "        print(\"Extracting unigrams...\")\n",
        "        self.unigram_matrix = self.vectorizer_unigram.fit_transform(self.documents)\n",
        "        self.unigram_feature_names = self.vectorizer_unigram.get_feature_names_out()\n",
        "        print(f\"Number of unigram features: {len(self.unigram_feature_names)}\")\n",
        "\n",
        "        print(\"Extracting bigrams...\")\n",
        "        self.bigram_matrix = self.vectorizer_bigram.fit_transform(self.documents)\n",
        "        self.bigram_feature_names = self.vectorizer_bigram.get_feature_names_out()\n",
        "        print(f\"Number of bigram features: {len(self.bigram_feature_names)}\")\n",
        "\n",
        "        print(\"Extracting combined unigrams and bigrams...\")\n",
        "        self.combined_matrix = self.vectorizer_combined.fit_transform(self.documents)\n",
        "        self.combined_feature_names = self.vectorizer_combined.get_feature_names_out()\n",
        "        print(f\"Number of combined unigram and bigram features: {len(self.combined_feature_names)}\")\n",
        "\n",
        "        return self.unigram_matrix, self.bigram_matrix, self.combined_matrix\n",
        "    \n",
        "    def save_features(self):\n",
        "        \"\"\"\n",
        "        Save unigram and bigram features with labels as CSV files.\n",
        "        \"\"\"\n",
        "        unigram_file = os.path.join(self.output_folder, \"unigram_features_with_labels.csv\")\n",
        "        bigram_file = os.path.join(self.output_folder, \"bigram_features_with_labels.csv\")\n",
        "\n",
        "        if not os.path.exists(unigram_file):\n",
        "            unigram_df = pd.DataFrame(self.unigram_matrix.toarray(), columns=self.unigram_feature_names)\n",
        "            unigram_df['label'] = self.labels\n",
        "            unigram_df.to_csv(unigram_file, index=False)\n",
        "            print(f\"Saved unigram features to {unigram_file}.\")\n",
        "        else:\n",
        "            print(f\"Unigram features file already exists at {unigram_file}.\")\n",
        "\n",
        "        if not os.path.exists(bigram_file):\n",
        "            bigram_df = pd.DataFrame(self.bigram_matrix.toarray(), columns=self.bigram_feature_names)\n",
        "            bigram_df['label'] = self.labels\n",
        "            bigram_df.to_csv(bigram_file, index=False)\n",
        "            print(f\"Saved bigram features to {bigram_file}.\")\n",
        "        else:\n",
        "            print(f\"Bigram features file already exists at {bigram_file}.\")\n",
        "\n",
        "    def get_top_features(self, feature_type=\"unigram\", top_n=10):\n",
        "        \"\"\"\n",
        "        Get the top N most common features for unigrams or bigrams based on TF-IDF scores.\n",
        "        \"\"\"\n",
        "        if feature_type == \"unigram\":\n",
        "            tfidf_sums = np.array(self.unigram_matrix.sum(axis=0)).flatten()\n",
        "            feature_names = self.unigram_feature_names\n",
        "        elif feature_type == \"bigram\":\n",
        "            tfidf_sums = np.array(self.bigram_matrix.sum(axis=0)).flatten()\n",
        "            feature_names = self.bigram_feature_names\n",
        "        else:\n",
        "            raise ValueError(\"Invalid feature_type. Choose 'unigram' or 'bigram'.\")\n",
        "\n",
        "        top_indices = np.argsort(tfidf_sums)[-top_n:]\n",
        "        print(f\"Top {top_n} Most Common {feature_type.capitalize()} Features:\")\n",
        "        for i in reversed(top_indices):\n",
        "            print(f\"{feature_names[i]}: {tfidf_sums[i]:.4f}\")\n",
        "\n",
        "    def train_model(self, feature_type=\"unigram\"):\n",
        "        \"\"\"\n",
        "        Train a Logistic Regression model using unigrams or bigrams.\n",
        "        \"\"\"\n",
        "        if feature_type == \"unigram\":\n",
        "            X = self.unigram_matrix\n",
        "        elif feature_type == \"bigram\":\n",
        "            X = self.bigram_matrix\n",
        "        else:\n",
        "            raise ValueError(\"Invalid feature_type. Choose 'unigram' or 'bigram'.\")\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, self.labels, test_size=0.2, random_state=42)\n",
        "        print(f\"Training set size: {X_train.shape}\")\n",
        "        print(f\"Testing set size: {X_test.shape}\")\n",
        "\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['liblinear']\n",
        "        }\n",
        "        grid_search = GridSearchCV(LogisticRegression(max_iter=500), param_grid, cv=5, scoring='accuracy')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(\"\\nBest Hyperparameters:\")\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        classifier = grid_search.best_estimator_\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "        return classifier\n",
        "\n",
        "    def compute_frequencies(self, feature_type=\"unigram\"):\n",
        "        \"\"\"\n",
        "        Compute frequencies of unigrams or bigrams for depression and non-depression posts.\n",
        "        \"\"\"\n",
        "        if feature_type == \"unigram\":\n",
        "            matrix = self.unigram_matrix\n",
        "            feature_names = self.unigram_feature_names\n",
        "        elif feature_type == \"bigram\":\n",
        "            matrix = self.bigram_matrix\n",
        "            feature_names = self.bigram_feature_names\n",
        "        else:\n",
        "            raise ValueError(\"Invalid feature_type. Choose 'unigram' or 'bigram'.\")\n",
        "\n",
        "        depression_indices = [i for i, label in enumerate(self.labels) if label == 1]\n",
        "        non_depression_indices = [i for i, label in enumerate(self.labels) if label == 0]\n",
        "\n",
        "        depression_matrix = matrix[depression_indices]\n",
        "        non_depression_matrix = matrix[non_depression_indices]\n",
        "\n",
        "        depression_sums = depression_matrix.sum(axis=0).A1\n",
        "        non_depression_sums = non_depression_matrix.sum(axis=0).A1\n",
        "\n",
        "        depression_freqs = {feature_names[i]: depression_sums[i] for i in range(len(feature_names))}\n",
        "        non_depression_freqs = {feature_names[i]: non_depression_sums[i] for i in range(len(feature_names))}\n",
        "\n",
        "        return depression_freqs, non_depression_freqs\n",
        "\n",
        "    def get_top_n_features(self, frequencies, top_n=100):\n",
        "        \"\"\"\n",
        "        Get the top N most frequent features from the computed frequencies.\n",
        "        \"\"\"\n",
        "        sorted_features = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_features[:top_n]\n",
        "\n",
        "    def visualize_wordcloud(self, frequencies, title, max_words=100):\n",
        "        \"\"\"\n",
        "        Generate a word cloud from frequencies.\n",
        "        \"\"\"\n",
        "        wordcloud = WordCloud(width=800, height=400, background_color='white', max_words=max_words)\n",
        "        wordcloud = wordcloud.generate_from_frequencies(frequencies)\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.imshow(wordcloud, interpolation='bilinear')\n",
        "        plt.axis('off')\n",
        "        plt.title(title, fontsize=16)\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "class EmpathFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, documents, labels, selected_categories, output_folder=\"data/feature_extracted_data\"):\n",
        "        \"\"\"\n",
        "        Initialize the EmpathFeatureExtractor class.\n",
        "\n",
        "        Parameters:\n",
        "        documents (list of str): List of documents to analyze.\n",
        "        labels (list of int): Labels corresponding to the documents.\n",
        "        selected_categories (dict): Categories for Empath analysis.\n",
        "        output_folder (str): Folder to save the results.\n",
        "        \"\"\"\n",
        "        super().__init__(documents, labels, output_folder)\n",
        "        self.lexicon = Empath()\n",
        "        self.categories = selected_categories\n",
        "        self.features = None\n",
        "        self.correlation_results = None\n",
        "        self.significant_results = None\n",
        "\n",
        "    def extract_empath_features(self):\n",
        "        \"\"\"\n",
        "        Extract Empath features based on the selected categories.\n",
        "        \"\"\"\n",
        "        features = []\n",
        "        for doc in self.documents:\n",
        "            doc_features = {}\n",
        "\n",
        "            # Linguistic features\n",
        "            for category in self.categories[\"linguistic_features\"]:\n",
        "                doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            # Psychological processes\n",
        "            for subcategory, categories in self.categories[\"psychological_processes\"].items():\n",
        "                for category in categories:\n",
        "                    doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            # Personal concerns\n",
        "            for category in self.categories[\"personal_concerns\"]:\n",
        "                doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            # Time orientations\n",
        "            for category in self.categories[\"time_orientations\"]:\n",
        "                doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            features.append(doc_features)\n",
        "\n",
        "        # Convert to a DataFrame\n",
        "        self.features = pd.DataFrame(features)\n",
        "        print(f\"Extracted Empath features with shape: {self.features.shape}\")\n",
        "\n",
        "    def analyze_correlation(self):\n",
        "        \"\"\"\n",
        "        Analyze correlations between extracted features and labels.\n",
        "        \"\"\"\n",
        "        if self.features is None:\n",
        "            raise ValueError(\"Features must be extracted before analyzing correlations.\")\n",
        "\n",
        "        # Remove constant columns\n",
        "        constant_columns = self.features.columns[self.features.nunique() == 1]\n",
        "        self.features = self.features.drop(columns=constant_columns)\n",
        "        print(f\"Removed constant columns: {constant_columns.tolist()}\")\n",
        "\n",
        "        # Validate labels\n",
        "        if len(set(self.labels)) == 1:\n",
        "            raise ValueError(\"Labels array is constant; cannot compute correlation.\")\n",
        "\n",
        "        correlations, p_values = [], []\n",
        "\n",
        "        for column in self.features.columns:\n",
        "            try:\n",
        "                # Calculate Pearson correlation and p-value\n",
        "                correlation, p_value = pearsonr(self.features[column], self.labels)\n",
        "                correlations.append(correlation)\n",
        "                p_values.append(p_value)\n",
        "            except ValueError as e:\n",
        "                print(f\"Skipping feature {column} due to constant input.\")\n",
        "                correlations.append(None)\n",
        "                p_values.append(None)\n",
        "\n",
        "        # Apply Benjamini-Hochberg correction\n",
        "        correction_results = multipletests(p_values, alpha=0.05, method=\"fdr_bh\")\n",
        "        _, corrected_p_values, _, _ = correction_results\n",
        "\n",
        "        # Create correlation results DataFrame\n",
        "        self.correlation_results = pd.DataFrame({\n",
        "            \"Feature\": self.features.columns,\n",
        "            \"Correlation\": correlations,\n",
        "            \"P-Value\": p_values,\n",
        "            \"Corrected P-Value\": corrected_p_values\n",
        "        }).sort_values(by=\"Correlation\", key=abs, ascending=False)\n",
        "\n",
        "    def filter_significant_results(self):\n",
        "        \"\"\"\n",
        "        Filter significant results based on corrected P-values.\n",
        "        \"\"\"\n",
        "        if self.correlation_results is None:\n",
        "            raise ValueError(\"Correlation analysis must be performed before filtering significant results.\")\n",
        "\n",
        "        def significance_stars(p):\n",
        "            if p < 0.001:\n",
        "                return '***'\n",
        "            elif p < 0.01:\n",
        "                return '**'\n",
        "            elif p < 0.05:\n",
        "                return '*'\n",
        "            return ''  # No significance\n",
        "\n",
        "        self.correlation_results[\"Significance\"] = self.correlation_results[\"Corrected P-Value\"].apply(significance_stars)\n",
        "        self.significant_results = self.correlation_results[self.correlation_results[\"Significance\"] != '']\n",
        "\n",
        "    def map_features_to_categories(self):\n",
        "        \"\"\"\n",
        "        Map significant features to their respective categories.\n",
        "        \"\"\"\n",
        "        if self.significant_results is None:\n",
        "            raise ValueError(\"Significant results must be filtered before mapping features to categories.\")\n",
        "\n",
        "        grouped_results = defaultdict(list)\n",
        "\n",
        "        # Iterate over features and map them to categories\n",
        "        for feature in self.significant_results[\"Feature\"]:\n",
        "            for category, subcategories in self.categories.items():\n",
        "                if isinstance(subcategories, list):  # Flat category\n",
        "                    if feature in subcategories:\n",
        "                        grouped_results[category].append(feature)\n",
        "                elif isinstance(subcategories, dict):  # Nested subcategories\n",
        "                    for subcategory, sub_features in subcategories.items():\n",
        "                        if feature in sub_features:\n",
        "                            grouped_results[f\"{category} - {subcategory}\"].append(feature)\n",
        "\n",
        "        # Format results into a DataFrame\n",
        "        formatted_results = [\n",
        "            {\n",
        "                \"Category\": category,\n",
        "                \"Feature\": feature,\n",
        "                \"Correlation\": self.significant_results.loc[self.significant_results[\"Feature\"] == feature, \"Correlation\"].values[0],\n",
        "                \"P-Value\": self.significant_results.loc[self.significant_results[\"Feature\"] == feature, \"P-Value\"].values[0],\n",
        "                \"Significance\": self.significant_results.loc[self.significant_results[\"Feature\"] == feature, \"Significance\"].values[0],\n",
        "            }\n",
        "            for category, features in grouped_results.items()\n",
        "            for feature in features\n",
        "        ]\n",
        "        return pd.DataFrame(formatted_results).sort_values(by=[\"Category\", \"Correlation\"], ascending=[True, False])\n",
        "\n",
        "    def save_features_and_results(self):\n",
        "        \"\"\"\n",
        "        Save extracted features and correlation results to CSV files.\n",
        "        \"\"\"\n",
        "        # Save features\n",
        "        if self.features is not None:\n",
        "            feature_file = \"empath_features_with_labels.csv\"\n",
        "            self.save_to_csv(self.features, feature_file)\n",
        "\n",
        "        # Save correlation results\n",
        "        if self.correlation_results is not None:\n",
        "            correlation_file = \"empath_correlation_results.csv\"\n",
        "            self.save_to_csv(self.correlation_results, correlation_file)\n",
        "\n",
        "    def visualize_category_correlations(self):\n",
        "        \"\"\"\n",
        "        Visualize average correlations for categories as a bar chart.\n",
        "        \"\"\"\n",
        "        if self.category_correlations is None:\n",
        "            raise ValueError(\"Category correlations must be computed before visualization.\")\n",
        "\n",
        "        categories = list(self.category_correlations.keys())\n",
        "        correlations = list(self.category_correlations.values())\n",
        "\n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.barh(categories, correlations, color='skyblue')\n",
        "        plt.xlabel(\"Average Correlation\")\n",
        "        plt.title(\"Average Correlation by Empath Categories\")\n",
        "        plt.gca().invert_yaxis()  # Invert y-axis for better readability\n",
        "        plt.show()\n",
        "\n",
        "# Empath Feature Analyzer\n",
        "class EmpathFeatureAnalyzer(EmpathFeatureExtractor):\n",
        "    def __init__(self, documents, labels, categories, output_folder=\"data/feature_extracted_data\"):\n",
        "        super().__init__(documents, labels, categories, output_folder)\n",
        "        self.category_correlations = {}\n",
        "\n",
        "    def group_correlations_by_subcategory(self):\n",
        "        \"\"\"\n",
        "        Group feature correlations by subcategories and calculate the average correlation for each group.\n",
        "        \"\"\"\n",
        "        if self.correlation_results is None:\n",
        "            raise ValueError(\"Correlation analysis must be performed before grouping.\")\n",
        "\n",
        "        grouped_results = defaultdict(list)\n",
        "\n",
        "        # Iterate through features, correlations, and P-values\n",
        "        for feature, correlation, p_value in zip(\n",
        "            self.correlation_results['Feature'],\n",
        "            self.correlation_results['Correlation'],\n",
        "            self.correlation_results['P-Value']\n",
        "        ):\n",
        "            for category, subcategories in self.categories.items():\n",
        "                if isinstance(subcategories, dict):  # Handle nested subcategories\n",
        "                    for subcategory, sub_features in subcategories.items():\n",
        "                        if feature in sub_features:\n",
        "                            grouped_results[f\"{category} - {subcategory}\"].append((feature, correlation, p_value))\n",
        "                elif isinstance(subcategories, list):  # Handle flat categories\n",
        "                    if feature in subcategories:\n",
        "                        grouped_results[category].append((feature, correlation, p_value))\n",
        "\n",
        "        # Store the top example words and average correlations\n",
        "        self.category_correlations = {}\n",
        "        for group, correlations in grouped_results.items():\n",
        "            avg_correlation = np.mean([c[1] for c in correlations])\n",
        "            sorted_features = sorted(correlations, key=lambda x: abs(x[1]), reverse=True)\n",
        "            top_feature = sorted_features[0]  # Select the most correlated feature as an example\n",
        "            self.category_correlations[group] = {\n",
        "                \"Example Word\": top_feature[0],\n",
        "                \"Correlation\": avg_correlation,\n",
        "                \"P-Value\": top_feature[2]\n",
        "            }\n",
        "\n",
        "    def generate_summary_table(self):\n",
        "        \"\"\"\n",
        "        Generate a summary table with categories, example words, correlations, and P-values.\n",
        "        \"\"\"\n",
        "        if not self.category_correlations:\n",
        "            raise ValueError(\"Category correlations must be computed before generating a summary table.\")\n",
        "\n",
        "        summary_data = []\n",
        "        for category, details in self.category_correlations.items():\n",
        "            summary_data.append({\n",
        "                \"LIWC Category\": category,\n",
        "                \"Example Word\": details[\"Example Word\"],\n",
        "                \"Correlation\": f\"{details['Correlation']:.2f}\",\n",
        "                \"P-Value\": f\"{details['P-Value']:.3f}\"\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        return summary_df\n",
        "\n",
        "    def visualize_summary_table(self):\n",
        "        \"\"\"\n",
        "        Display the summary table in the notebook.\n",
        "        \"\"\"\n",
        "        summary_table = self.generate_summary_table()\n",
        "        print(summary_table.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LDA Feature Extractor\n",
        "class LDAFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, documents, labels, num_topics=30, passes=15, output_folder=\"data/feature_extracted_data\", random_state=42):\n",
        "        super().__init__(documents, labels, output_folder)\n",
        "        self.num_topics = num_topics\n",
        "        self.passes = passes\n",
        "        self.random_state = random_state\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.lda_model = None\n",
        "        self.topic_distributions = None\n",
        "        self.tsne_results = None\n",
        "\n",
        "    def preprocess_documents(self):\n",
        "        \"\"\"\n",
        "        Preprocess documents: tokenize, remove stopwords, and stem.\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stemmer = PorterStemmer()\n",
        "        processed_docs = [\n",
        "            [\n",
        "                stemmer.stem(word) for word in word_tokenize(doc.lower())\n",
        "                if word.isalpha() and word not in stop_words\n",
        "            ]\n",
        "            for doc in self.documents\n",
        "        ]\n",
        "        return processed_docs\n",
        "\n",
        "    def filter_docs_by_word_count(self, processed_docs, min_documents=10):\n",
        "        \"\"\"\n",
        "        Filter words that appear in more than 10 documents.\n",
        "        \"\"\"\n",
        "        word_doc_count = defaultdict(int)\n",
        "        for doc in processed_docs:\n",
        "            unique_words = set(doc)\n",
        "            for word in unique_words:\n",
        "                word_doc_count[word] += 1\n",
        "\n",
        "        filtered_docs = [\n",
        "            [word for word in doc if word_doc_count[word] > min_documents]\n",
        "            for doc in processed_docs\n",
        "        ]\n",
        "        return filtered_docs\n",
        "\n",
        "    def train_lda(self, processed_docs):\n",
        "        \"\"\"\n",
        "        Train the LDA model.\n",
        "        \"\"\"\n",
        "        self.dictionary = corpora.Dictionary(processed_docs)\n",
        "        self.corpus = [self.dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "        self.lda_model = LdaModel(\n",
        "            self.corpus, num_topics=self.num_topics, id2word=self.dictionary, passes=self.passes,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "    def extract_topic_distributions(self):\n",
        "        \"\"\"\n",
        "        Extract topic distributions for each document.\n",
        "        \"\"\"\n",
        "        self.topic_distributions = [\n",
        "            dict(self.lda_model.get_document_topics(doc, minimum_probability=0))\n",
        "            for doc in self.corpus\n",
        "        ]\n",
        "\n",
        "    def topic_distribution_to_matrix(self):\n",
        "        \"\"\"\n",
        "        Convert topic distributions to a matrix format.\n",
        "        \"\"\"\n",
        "        matrix = np.zeros((len(self.topic_distributions), self.num_topics))\n",
        "        for i, distribution in enumerate(self.topic_distributions):\n",
        "            for topic_id, prob in distribution.items():\n",
        "                matrix[i, topic_id] = prob\n",
        "        return matrix\n",
        "\n",
        "    def visualize_lda(self, label_filter=None):\n",
        "        \"\"\"\n",
        "        Visualize the LDA model with pyLDAvis, with an option to filter by depressed or non-depressed posts.\n",
        "\n",
        "        Parameters:\n",
        "        label_filter (int, optional): Filter by label. Use 1 for depressed, 0 for non-depressed. \n",
        "                                       If None, visualize all posts.\n",
        "        \"\"\"\n",
        "        if not self.lda_model or not self.dictionary:\n",
        "            raise ValueError(\"LDA model or dictionary not available. Train the model first.\")\n",
        "        \n",
        "        # Filtered visualization\n",
        "        if label_filter is not None:\n",
        "            filtered_docs = [doc for doc, label in zip(self.documents, self.labels) if label == label_filter]\n",
        "            print(f\"Generating LDA visualization for {'depressed' if label_filter == 1 else 'non-depressed'} posts...\")\n",
        "\n",
        "            # Preprocess the filtered documents\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            stemmer = PorterStemmer()\n",
        "            processed_docs = [\n",
        "                [\n",
        "                    stemmer.stem(word) for word in word_tokenize(doc.lower())\n",
        "                    if word.isalpha() and word not in stop_words\n",
        "                ]\n",
        "                for doc in filtered_docs\n",
        "            ]\n",
        "            \n",
        "            # Create a corpus for the filtered documents\n",
        "            filtered_corpus = [self.dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "            vis = gensimvis.prepare(self.lda_model, filtered_corpus, self.dictionary)\n",
        "        else:\n",
        "            # Visualization for all posts\n",
        "            print(\"Generating LDA visualization for all posts...\")\n",
        "            vis = gensimvis.prepare(self.lda_model, self.corpus, self.dictionary)\n",
        "\n",
        "        # Ensure pyLDAvis works in the notebook\n",
        "        pyLDAvis.enable_notebook()  # Enable visualization in Jupyter Notebook\n",
        "\n",
        "        # Display visualization\n",
        "        return pyLDAvis.display(vis)\n",
        "\n",
        "    def visualize_tsne(self, matrix):\n",
        "        \"\"\"\n",
        "        Visualize topic distributions using t-SNE.\n",
        "        \"\"\"\n",
        "        labels = self.labels\n",
        "        tsne = TSNE(n_components=2, perplexity=50, n_iter=500, random_state=self.random_state)\n",
        "        self.tsne_results = tsne.fit_transform(matrix)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        scatter = plt.scatter(\n",
        "            self.tsne_results[:, 0], self.tsne_results[:, 1], c=labels, cmap='viridis', s=10, alpha=0.7\n",
        "        )\n",
        "        plt.legend(handles=scatter.legend_elements()[0], labels=[\"Non-Depressed\", \"Depressed\"], title=\"Labels\")\n",
        "        plt.title(\"t-SNE Visualization of LDA Topic Distributions\")\n",
        "        plt.xlabel(\"Dimension 1\")\n",
        "        plt.ylabel(\"Dimension 2\")\n",
        "        plt.show()\n",
        "\n",
        "    def save_features(self):\n",
        "        \"\"\"\n",
        "        Save LDA topic distributions with labels to a CSV file.\n",
        "        \"\"\"\n",
        "        if not self.topic_distributions:\n",
        "            raise ValueError(\"Topic distributions are not extracted.\")\n",
        "\n",
        "        # Prepare the LDA features data\n",
        "        topic_matrix = self.topic_distribution_to_matrix()\n",
        "        labels_array = np.array(self.labels)\n",
        "        lda_features_df = pd.DataFrame(topic_matrix)\n",
        "        lda_features_df['label'] = labels_array\n",
        "\n",
        "        # Define the filename\n",
        "        lda_features_file = \"lda_topic_distributions_with_labels.csv\"\n",
        "\n",
        "        # Call the base class method for saving\n",
        "        self.save_to_csv(lda_features_df, lda_features_file)\n",
        "        \n",
        "    def run_pipeline(self):\n",
        "        \"\"\"\n",
        "        Complete LDA pipeline: preprocess, train, extract, visualize, and save.\n",
        "        \"\"\"\n",
        "        print(\"Preprocessing documents...\")\n",
        "        processed_docs = self.preprocess_documents()\n",
        "        filtered_docs = self.filter_docs_by_word_count(processed_docs)\n",
        "\n",
        "        print(\"Training LDA model...\")\n",
        "        self.train_lda(filtered_docs)\n",
        "\n",
        "        print(\"Extracting topic distributions...\")\n",
        "        self.extract_topic_distributions()\n",
        "\n",
        "        print(\"Saving features...\")\n",
        "        self.save_features()\n",
        "\n",
        "        print(\"Visualizing t-SNE...\")\n",
        "        topic_matrix = self.topic_distribution_to_matrix()\n",
        "        self.visualize_tsne(topic_matrix)\n",
        "\n",
        "        print(\"LDA pipeline complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 293 documents.\n",
            "Labels: {0, 1}\n"
          ]
        }
      ],
      "source": [
        "# Loading data\n",
        "if __name__ == \"__main__\":\n",
        "    # Load documents and labels\n",
        "    folders = {\n",
        "        \"depression\": {\"path\": \"data/preprocessed/preprocessed_depression_posts\", \"label\": 1},\n",
        "        \"breastcancer\": {\"path\": \"data/preprocessed/preprocessed_breastcancer_posts\", \"label\": 0},\n",
        "    }\n",
        "    documents, labels = [], []\n",
        "    for category, data in folders.items():\n",
        "        for file_name in os.listdir(data[\"path\"]):\n",
        "            file_path = os.path.join(data[\"path\"], file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                documents.append(file.read())\n",
        "                labels.append(data[\"label\"])\n",
        "    print(f\"Loaded {len(documents)} documents.\")\n",
        "    print(f\"Labels: {set(labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-Gram Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting unigrams...\n",
            "Number of unigram features: 4296\n",
            "Extracting bigrams...\n",
            "Number of bigram features: 29708\n",
            "Extracting combined unigrams and bigrams...\n",
            "Number of combined unigram and bigram features: 34004\n",
            "Unigram features file already exists at data/feature_extracted_data/unigram_features_with_labels.csv.\n",
            "Bigram features file already exists at data/feature_extracted_data/bigram_features_with_labels.csv.\n",
            "\n",
            "Top Unigram Features:\n",
            "Top 10 Most Common Unigram Features:\n",
            "feel: 19.7904\n",
            "like: 17.1393\n",
            "want: 13.3932\n",
            "know: 12.1230\n",
            "year: 11.2671\n",
            "life: 10.9066\n",
            "time: 10.2350\n",
            "realli: 9.5660\n",
            "thing: 8.9140\n",
            "friend: 8.8003\n",
            "\n",
            "Top Bigram Features:\n",
            "Top 10 Most Common Bigram Features:\n",
            "feel like: 7.1419\n",
            "year old: 1.9008\n",
            "mental health: 1.7841\n",
            "anyon els: 1.7453\n",
            "wan na: 1.6263\n",
            "lymph node: 1.6214\n",
            "year ago: 1.5248\n",
            "everi day: 1.5186\n",
            "gon na: 1.4851\n",
            "realli want: 1.3432\n",
            "\n",
            "Training a model with unigram features...\n",
            "Training set size: (234, 4296)\n",
            "Testing set size: (59, 4296)\n",
            "\n",
            "Best Hyperparameters:\n",
            "{'C': 100, 'penalty': 'l2', 'solver': 'liblinear'}\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.57      0.73        14\n",
            "           1       0.88      1.00      0.94        45\n",
            "\n",
            "    accuracy                           0.90        59\n",
            "   macro avg       0.94      0.79      0.83        59\n",
            "weighted avg       0.91      0.90      0.89        59\n",
            "\n",
            "Accuracy: 0.90\n",
            "\n",
            "Generating Word Clouds...\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "NGramFeatureExtractor.visualize_wordcloud() got an unexpected keyword argument 'feature_type'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 24\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Generate word clouds\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mGenerating Word Clouds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 24\u001b[0m \u001b[43mngram_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvisualize_wordcloud\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43munigram\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtitle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDepression Word Cloud\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m ngram_extractor\u001b[38;5;241m.\u001b[39mvisualize_wordcloud(feature_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbigram\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-Depression Word Cloud\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m ngram_extractor\u001b[38;5;241m.\u001b[39mvisualize_wordcloud(feature_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcombined\u001b[39m\u001b[38;5;124m\"\u001b[39m, title\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCombined Word Cloud\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: NGramFeatureExtractor.visualize_wordcloud() got an unexpected keyword argument 'feature_type'"
          ]
        }
      ],
      "source": [
        "# Initialize, extract, save, analyze, train and generate word clouds.\n",
        "# Initialize the n-gram extractor\n",
        "ngram_extractor = NGramFeatureExtractor(documents, labels)\n",
        "\n",
        "# Extract features\n",
        "ngram_extractor.extract_features()\n",
        "\n",
        "# Save features\n",
        "ngram_extractor.save_features()\n",
        "\n",
        "# Analyze top features\n",
        "print(\"\\nTop Unigram Features:\")\n",
        "ngram_extractor.get_top_features(feature_type=\"unigram\", top_n=10)\n",
        "print(\"\\nTop Bigram Features:\")\n",
        "ngram_extractor.get_top_features(feature_type=\"bigram\", top_n=10)\n",
        "\n",
        "\n",
        "# Train a model\n",
        "print(\"\\nTraining a model with unigram features...\")\n",
        "classifier = ngram_extractor.train_model(feature_type=\"unigram\")\n",
        "\n",
        "# Generate word clouds\n",
        "print(\"\\nGenerating Word Clouds...\")\n",
        "ngram_extractor.visualize_wordcloud(feature_type=\"unigram\", title=\"Depression Word Cloud\")\n",
        "ngram_extractor.visualize_wordcloud(feature_type=\"bigram\", title=\"Non-Depression Word Cloud\")\n",
        "ngram_extractor.visualize_wordcloud(feature_type=\"combined\", title=\"Combined Word Cloud\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "870CuQF6-sNC"
      },
      "source": [
        "### Extract TF-IDF features, train a logistic regression model with hyperparameter tuning, and identify key features for each category."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Debugging: Check documents and labels\n",
        "print(f\"Number of valid documents: {len(documents)}\")\n",
        "if documents:\n",
        "    print(f\"Sample document (label {labels[0]}): {documents[0]}\")\n",
        "\n",
        "# Initialize TfidfVectorizer with unigrams and bigrams\n",
        "vectorizer = TfidfVectorizer(ngram_range=(1, 2), stop_words='english')\n",
        "tfidf_matrix = vectorizer.fit_transform(documents)\n",
        "\n",
        "print(f\"TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
        "\n",
        "# Split data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(tfidf_matrix, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Testing set size: {X_test.shape}\")\n",
        "\n",
        "# Hyperparameter Tuning with GridSearchCV\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10, 100],  # Regularization strength\n",
        "    'penalty': ['l2'],  # L1 is not supported by LogisticRegression with liblinear solver\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=500), param_grid, cv=5, scoring='accuracy')\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"\\nBest Hyperparameters:\")\n",
        "print(grid_search.best_params_)\n",
        "\n",
        "# Train the best Logistic Regression model\n",
        "classifier = grid_search.best_estimator_\n",
        "classifier.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = classifier.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "# Get feature names and coefficients\n",
        "feature_names = vectorizer.get_feature_names_out()\n",
        "coefficients = classifier.coef_[0]  # No need to call .toarray() since coef_ is already a numpy array\n",
        "\n",
        "# Get top 10 positive and negative features\n",
        "top_positive_indices = np.argsort(coefficients)[-10:]  # Largest coefficients\n",
        "top_negative_indices = np.argsort(coefficients)[:10]   # Smallest coefficients\n",
        "\n",
        "print(\"\\nTop 10 Positive Features (indicating depression):\")\n",
        "for i in reversed(top_positive_indices):\n",
        "    print(f\"{feature_names[i]}: {coefficients[i]:.4f}\")\n",
        "\n",
        "print(\"\\nTop 10 Negative Features (indicating breast cancer):\")\n",
        "for i in top_negative_indices:\n",
        "    print(f\"{feature_names[i]}: {coefficients[i]:.4f}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def visualize_features(feature_names, coefficients, title, top_n=10):\n",
        "    sorted_indices = np.argsort(coefficients)\n",
        "    top_features = [(feature_names[i], coefficients[i]) for i in sorted_indices[-top_n:]]\n",
        "    bottom_features = [(feature_names[i], coefficients[i]) for i in sorted_indices[:top_n]]\n",
        "    \n",
        "    top_features.reverse()  # Largest first\n",
        "    bottom_features.reverse()  # Smallest first\n",
        "    \n",
        "    # Plot positive features\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh([f[0] for f in top_features], [f[1] for f in top_features], color='blue')\n",
        "    plt.xlabel(\"Coefficient Value\")\n",
        "    plt.title(f\"Top {top_n} {title} Features (Positive Class)\")\n",
        "    plt.show()\n",
        "    \n",
        "    # Plot negative features\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.barh([f[0] for f in bottom_features], [f[1] for f in bottom_features], color='red')\n",
        "    plt.xlabel(\"Coefficient Value\")\n",
        "    plt.title(f\"Top {top_n} {title} Features (Negative Class)\")\n",
        "    plt.show()\n",
        "\n",
        "visualize_features(feature_names, coefficients, \"Depression vs Breast Cancer\", top_n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Empath Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'selected_categories' (dict)\n"
          ]
        }
      ],
      "source": [
        "# Categories to focus on based on the origin paper. \n",
        "# The categories are divided into linguistic features, psychological processes, personal concerns, and time orientations.\n",
        "# Since we cannot use the LIWC tool, we will use the Empath tool and define similar categorical features. \n",
        "\n",
        "selected_categories = {\n",
        "    \"linguistic_features\": [\n",
        "        \"articles\", \"auxiliary_verbs\", \"adverbs\", \"conjunctions\", \n",
        "        \"personal_pronouns\", \"impersonal_pronouns\", \"negations\", \n",
        "        \"prepositions\", \"verbs\", \"nouns\", \"adjectives\", \n",
        "        \"comparatives\", \"superlatives\", \"modifiers\", \"function_words\", \n",
        "        \"filler_words\", \"verb_tense\", \"slang\", \"jargon\", \n",
        "        \"formal_language\", \"casual_language\", \"exclamations\", \n",
        "        \"contractions\", \"word_complexity\", \"sentiment_words\"\n",
        "    ],\n",
        "        \"psychological_processes\": {\n",
        "        \"affective\": [\n",
        "            \"positive_emotion\", \"negative_emotion\", \"joy\", \"anger\", \n",
        "            \"sadness\", \"anxiety\", \"fear\", \"disgust\", \"love\", \n",
        "            \"hope\", \"trust\", \"excitement\", \"anticipation\", \n",
        "            \"relief\", \"sympathy\", \"gratitude\", \"shame\", \n",
        "            \"guilt\", \"envy\", \"pride\", \"contentment\", \"confusion\",\n",
        "            \"boredom\", \"embarrassment\", \"longing\", \"nostalgia\", \n",
        "            \"embarrassment\", \"frustration\", \"surprise\", \"melancholy\"\n",
        "        ],\n",
        "        \"biological\": [\n",
        "            \"body\", \"health\", \"illness\", \"pain\", \"hygiene\", \n",
        "            \"fitness\", \"exercise\", \"nutrition\", \"ingestion\", \n",
        "            \"physical_state\", \"medicine\", \"sleep\", \"sexual\", \n",
        "            \"aging\", \"disease\", \"injury\", \"hospital\", \"recovery\", \n",
        "            \"dieting\", \"mental_health\", \"drug_use\", \"headache\", \n",
        "            \"fatigue\", \"hormones\", \"appetite\"\n",
        "        ],\n",
        "        \"social\": [\n",
        "            \"family\", \"friends\", \"relationships\", \"group_behavior\", \n",
        "            \"teamwork\", \"social_media\", \"communication\", \"community\", \n",
        "            \"peer_pressure\", \"leadership\", \"parenting\", \"mentorship\", \n",
        "            \"marriage\", \"divorce\", \"gender_roles\", \"social_identity\", \n",
        "            \"cultural_rituals\", \"networking\", \"altruism\", \"conflict\", \n",
        "            \"social_support\", \"dominance\", \"affiliation\", \"intimacy\", \n",
        "            \"supportiveness\", \"competition\", \"conflict_resolution\", \n",
        "            \"collaboration\", \"in-group\", \"out-group\", \"prejudice\"\n",
        "        ],\n",
        "        \"cognitive\": [\n",
        "            \"certainty\", \"doubt\", \"insight\", \"cause\", \"discrepancy\", \n",
        "            \"problem_solving\", \"creativity\", \"self_reflection\", \"planning\", \n",
        "            \"memory\", \"perception\", \"attention\", \"reasoning\", \"thought_process\", \n",
        "            \"decision_making\", \"confusion\", \"learning\", \"metacognition\", \"adaptability\", \n",
        "            \"focus\", \"perspective\", \"problem_analysis\", \"evaluation\", \"interpretation\",\n",
        "            \"logic\", \"intelligence\", \"rational_thought\", \"intuition\", \"conceptualization\"\n",
        "        ],\n",
        "        \"drives\": [\n",
        "            \"achievement\", \"dominance\", \"affiliation\", \"control\", \n",
        "            \"self-esteem\", \"autonomy\", \"self-assertion\", \"power\", \n",
        "            \"ambition\", \"conformity\", \"subordination\", \"dependence\", \n",
        "            \"submission\", \"accomplishment\", \"independence\", \"order\", \n",
        "            \"control_seeking\", \"status\", \"prosocial_behavior\"\n",
        "        ],\n",
        "        \"spiritual\": [\n",
        "            \"spirituality\", \"faith\", \"beliefs\", \"sacred\", \"religion\", \n",
        "            \"prayer\", \"meditation\", \"afterlife\", \"soul\", \"divine\", \n",
        "            \"god\", \"higher_power\", \"inspiration\", \"transcendence\", \n",
        "            \"morality\", \"ethics\", \"rituals\", \"holiness\", \"mindfulness\"\n",
        "        ]\n",
        "    },\n",
        "    \"personal_concerns\": [\n",
        "        \"work\", \"money\", \"wealth\", \"shopping\", \"career\", \"travel\", \n",
        "        \"home\", \"school\", \"education\", \"violence\", \"death\", \n",
        "        \"retirement\", \"spirituality\", \"family_life\", \"hobbies\", \n",
        "        \"volunteering\", \"pets\", \"entertainment\", \"parenting\", \n",
        "        \"sports\", \"adventure\", \"politics\", \"environment\", \n",
        "        \"safety\", \"technology\", \"materialism\", \"status\", \n",
        "        \"self_improvement\", \"learning\", \"self_growth\", \"happiness\", \n",
        "        \"life_purpose\", \"work_life_balance\", \"stress\", \"coping\", \n",
        "        \"job_satisfaction\", \"ambition\", \"legacy\", \"job_search\", \n",
        "        \"unemployment\", \"retirement_plans\", \"mental_health\", \"dating\", \n",
        "        \"romantic_relationships\", \"divorce\", \"life_stressors\", \"transitions\"\n",
        "    ],\n",
        "    \"time_orientations\": [\n",
        "        \"present\", \"past\", \"future\", \"morning\", \n",
        "        \"afternoon\", \"evening\", \"day\", \"night\", \n",
        "        \"weekdays\", \"weekends\", \"seasons\", \"holidays\", \n",
        "        \"lifespan\", \"long_term\", \"short_term\", \n",
        "        \"routine\", \"historical\", \"epoch\", \"momentary\", \n",
        "        \"timeliness\", \"timelessness\", \"urgency\", \n",
        "        \"progression\", \"nostalgia\", \"anticipation\"\n",
        "    ]\n",
        "}\n",
        "%store selected_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize Empath Feature Extractor\n",
        "empath_extractor = EmpathFeatureExtractor(documents, labels, selected_categories)\n",
        "\n",
        "# Extract features\n",
        "empath_extractor.extract_empath_features()\n",
        "\n",
        "# Analyze correlations\n",
        "empath_extractor.analyze_correlation()\n",
        "\n",
        "# Filter significant results\n",
        "empath_extractor.filter_significant_results()\n",
        "\n",
        "# Map features to categories\n",
        "formatted_results = empath_extractor.map_features_to_categories()\n",
        "print(formatted_results)\n",
        "\n",
        "# Save features and results\n",
        "empath_extractor.save_features_and_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBY4UR2DAEqD"
      },
      "source": [
        "# LDA (Latent Dirichlet Allocation) Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "lda_extractor = LDAFeatureExtractor(documents, labels)\n",
        "lda_extractor.run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualising depression\n",
        "lda_extractor.visualize_lda(label_filter=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualising non-depression\n",
        "lda_extractor.visualize_lda(label_filter=0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Outcomes: 30 topics using LDA was appropriate instead of the 70 which the original paper found "
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSH8TbJAtGxRwhwTt6MCJf",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
