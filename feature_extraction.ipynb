{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunflowerseed17/PythonDSProject/blob/main/Feature_Extraction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports and Loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/nataszasiwinska/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     /Users/nataszasiwinska/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from empath import Empath\n",
        "from gensim import corpora\n",
        "from gensim.models import LdaModel\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from scipy.stats import pearsonr\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.manifold import TSNE\n",
        "from matplotlib import pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "from statsmodels.stats.multitest import multipletests\n",
        "import pyLDAvis\n",
        "import pyLDAvis.gensim_models as gensimvis\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Base Class\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, documents, labels, output_folder=\"data/feature_extracted_data\"):\n",
        "        self.documents = documents\n",
        "        self.labels = labels\n",
        "        self.output_folder = output_folder\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "\n",
        "    def preprocess_text(self, text):\n",
        "        \"\"\"\n",
        "        Tokenize, lowercase, remove stopwords, and stem.\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stemmer = PorterStemmer()\n",
        "        tokens = word_tokenize(text.lower())\n",
        "        return [stemmer.stem(word) for word in tokens if word.isalpha() and word not in stop_words]\n",
        "\n",
        "    def save_to_csv(self, data, filename):\n",
        "        \"\"\"\n",
        "        Save data to a CSV file.\n",
        "        \"\"\"\n",
        "        # Construct the full file path\n",
        "        if not filename.startswith(self.output_folder):\n",
        "            file_path = os.path.join(self.output_folder, filename)\n",
        "        else:\n",
        "            file_path = filename\n",
        "\n",
        "        # Debugging: Print the path being used\n",
        "        print(f\"Saving file to: {file_path}\")\n",
        "\n",
        "        # Ensure the directory exists\n",
        "        os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
        "\n",
        "        # Save the file if it doesn't already exist\n",
        "        if not os.path.exists(file_path):\n",
        "            data.to_csv(file_path, index=False)\n",
        "            print(f\"Saved to {file_path}.\")\n",
        "        else:\n",
        "            print(f\"File already exists at {file_path}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# N-Gram Feature Extractor\n",
        "class NGramFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, documents, labels, output_folder=\"data/feature_extracted_data\"):\n",
        "        super().__init__(documents, labels, output_folder)\n",
        "        self.vectorizer_unigram = TfidfVectorizer(ngram_range=(1, 1), stop_words='english')\n",
        "        self.vectorizer_bigram = TfidfVectorizer(ngram_range=(2, 2), stop_words='english')\n",
        "        self.vectorizer_combined = TfidfVectorizer(ngram_range=(1, 2), stop_words='english') \n",
        "        self.unigram_matrix = None\n",
        "        self.bigram_matrix = None\n",
        "        self.unigram_feature_names = None\n",
        "        self.bigram_feature_names = None\n",
        "        self.combined_matrix = None\n",
        "        self.combined_feature_names = None\n",
        "\n",
        "        os.makedirs(self.output_folder, exist_ok=True)\n",
        "\n",
        "    def extract_features(self):\n",
        "        \"\"\"\n",
        "        Extract unigram and bigram features using TF-IDF.\n",
        "        \"\"\"\n",
        "        print(\"Extracting unigrams...\")\n",
        "        self.unigram_matrix = self.vectorizer_unigram.fit_transform(self.documents)\n",
        "        self.unigram_feature_names = self.vectorizer_unigram.get_feature_names_out()\n",
        "        print(f\"Number of unigram features: {len(self.unigram_feature_names)}\")\n",
        "\n",
        "        print(\"Extracting bigrams...\")\n",
        "        self.bigram_matrix = self.vectorizer_bigram.fit_transform(self.documents)\n",
        "        self.bigram_feature_names = self.vectorizer_bigram.get_feature_names_out()\n",
        "        print(f\"Number of bigram features: {len(self.bigram_feature_names)}\")\n",
        "\n",
        "        print(\"Extracting combined unigrams and bigrams...\")\n",
        "        self.combined_matrix = self.vectorizer_combined.fit_transform(self.documents)\n",
        "        self.combined_feature_names = self.vectorizer_combined.get_feature_names_out()\n",
        "        print(f\"Number of combined unigram and bigram features: {len(self.combined_feature_names)}\")\n",
        "\n",
        "        return self.unigram_matrix, self.bigram_matrix, self.combined_matrix\n",
        "    \n",
        "    def save_features(self):\n",
        "        \"\"\"\n",
        "        Save unigram and bigram features with labels as CSV files.\n",
        "        \"\"\"\n",
        "        unigram_file = os.path.join(self.output_folder, \"unigram_features_with_labels.csv\")\n",
        "        bigram_file = os.path.join(self.output_folder, \"bigram_features_with_labels.csv\")\n",
        "\n",
        "        if not os.path.exists(unigram_file):\n",
        "            unigram_df = pd.DataFrame(self.unigram_matrix.toarray(), columns=self.unigram_feature_names)\n",
        "            unigram_df['label'] = self.labels\n",
        "            unigram_df.to_csv(unigram_file, index=False)\n",
        "            print(f\"Saved unigram features to {unigram_file}.\")\n",
        "        else:\n",
        "            print(f\"Unigram features file already exists at {unigram_file}.\")\n",
        "\n",
        "        if not os.path.exists(bigram_file):\n",
        "            bigram_df = pd.DataFrame(self.bigram_matrix.toarray(), columns=self.bigram_feature_names)\n",
        "            bigram_df['label'] = self.labels\n",
        "            bigram_df.to_csv(bigram_file, index=False)\n",
        "            print(f\"Saved bigram features to {bigram_file}.\")\n",
        "        else:\n",
        "            print(f\"Bigram features file already exists at {bigram_file}.\")\n",
        "\n",
        "    def get_top_features(self, feature_type=\"unigram\", top_n=10):\n",
        "        \"\"\"\n",
        "        Get the top N most common features for unigrams or bigrams based on TF-IDF scores.\n",
        "        \"\"\"\n",
        "        if feature_type == \"unigram\":\n",
        "            tfidf_sums = np.array(self.unigram_matrix.sum(axis=0)).flatten()\n",
        "            feature_names = self.unigram_feature_names\n",
        "        elif feature_type == \"bigram\":\n",
        "            tfidf_sums = np.array(self.bigram_matrix.sum(axis=0)).flatten()\n",
        "            feature_names = self.bigram_feature_names\n",
        "        else:\n",
        "            raise ValueError(\"Invalid feature_type. Choose 'unigram' or 'bigram'.\")\n",
        "\n",
        "        top_indices = np.argsort(tfidf_sums)[-top_n:]\n",
        "        print(f\"Top {top_n} Most Common {feature_type.capitalize()} Features:\")\n",
        "        for i in reversed(top_indices):\n",
        "            print(f\"{feature_names[i]}: {tfidf_sums[i]:.4f}\")\n",
        "\n",
        "    def train_model(self, feature_type=\"unigram\"):\n",
        "        \"\"\"\n",
        "        Train a Logistic Regression model using unigrams or bigrams.\n",
        "        \"\"\"\n",
        "        if feature_type == \"unigram\":\n",
        "            X = self.unigram_matrix\n",
        "        elif feature_type == \"bigram\":\n",
        "            X = self.bigram_matrix\n",
        "        else:\n",
        "            raise ValueError(\"Invalid feature_type. Choose 'unigram' or 'bigram'.\")\n",
        "\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, self.labels, test_size=0.2, random_state=42)\n",
        "        print(f\"Training set size: {X_train.shape}\")\n",
        "        print(f\"Testing set size: {X_test.shape}\")\n",
        "\n",
        "        param_grid = {\n",
        "            'C': [0.1, 1, 10, 100],\n",
        "            'penalty': ['l2'],\n",
        "            'solver': ['liblinear']\n",
        "        }\n",
        "        grid_search = GridSearchCV(LogisticRegression(max_iter=500), param_grid, cv=5, scoring='accuracy')\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "        print(\"\\nBest Hyperparameters:\")\n",
        "        print(grid_search.best_params_)\n",
        "\n",
        "        classifier = grid_search.best_estimator_\n",
        "        classifier.fit(X_train, y_train)\n",
        "        y_pred = classifier.predict(X_test)\n",
        "\n",
        "        print(\"\\nClassification Report:\")\n",
        "        print(classification_report(y_test, y_pred))\n",
        "        print(f\"Accuracy: {accuracy_score(y_test, y_pred):.2f}\")\n",
        "\n",
        "        return classifier\n",
        "\n",
        "    def compute_frequencies(self, feature_type=\"unigram\"):\n",
        "        \"\"\"\n",
        "        Compute frequencies of unigrams or bigrams for depression and non-depression posts.\n",
        "        \"\"\"\n",
        "        if feature_type == \"unigram\":\n",
        "            matrix = self.unigram_matrix\n",
        "            feature_names = self.unigram_feature_names\n",
        "        elif feature_type == \"bigram\":\n",
        "            matrix = self.bigram_matrix\n",
        "            feature_names = self.bigram_feature_names\n",
        "        else:\n",
        "            raise ValueError(\"Invalid feature_type. Choose 'unigram' or 'bigram'.\")\n",
        "\n",
        "        depression_indices = [i for i, label in enumerate(self.labels) if label == 1]\n",
        "        non_depression_indices = [i for i, label in enumerate(self.labels) if label == 0]\n",
        "\n",
        "        depression_matrix = matrix[depression_indices]\n",
        "        non_depression_matrix = matrix[non_depression_indices]\n",
        "\n",
        "        depression_sums = depression_matrix.sum(axis=0).A1\n",
        "        non_depression_sums = non_depression_matrix.sum(axis=0).A1\n",
        "\n",
        "        depression_freqs = {feature_names[i]: depression_sums[i] for i in range(len(feature_names))}\n",
        "        non_depression_freqs = {feature_names[i]: non_depression_sums[i] for i in range(len(feature_names))}\n",
        "\n",
        "        return depression_freqs, non_depression_freqs\n",
        "\n",
        "    def get_top_n_features(self, frequencies, top_n=100):\n",
        "        \"\"\"\n",
        "        Get the top N most frequent features from the computed frequencies.\n",
        "        \"\"\"\n",
        "        sorted_features = sorted(frequencies.items(), key=lambda x: x[1], reverse=True)\n",
        "        return sorted_features[:top_n]\n",
        "\n",
        "    def generate_wordclouds(self):\n",
        "        \"\"\"\n",
        "        Generate 4 word clouds:\n",
        "        - Unigrams for depression\n",
        "        - Bigrams for depression\n",
        "        - Unigrams for non-depression\n",
        "        - Bigrams for non-depression\n",
        "        \"\"\"\n",
        "        # Compute frequencies for depression and non-depression for unigrams and bigrams\n",
        "        depression_unigrams, non_depression_unigrams = self.compute_frequencies(feature_type=\"unigram\")\n",
        "        depression_bigrams, non_depression_bigrams = self.compute_frequencies(feature_type=\"bigram\")\n",
        "\n",
        "        # Combine word clouds into a dictionary for iteration\n",
        "        wordcloud_data = {\n",
        "            \"Depression - Unigrams\": depression_unigrams,\n",
        "            \"Depression - Bigrams\": depression_bigrams,\n",
        "            \"Non-Depression - Unigrams\": non_depression_unigrams,\n",
        "            \"Non-Depression - Bigrams\": non_depression_bigrams\n",
        "        }\n",
        "\n",
        "        # Generate and display each word cloud\n",
        "        for title, frequencies in wordcloud_data.items():\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(frequencies)\n",
        "            plt.figure(figsize=(10, 6))\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis('off')\n",
        "            plt.title(title, fontsize=16)\n",
        "            plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Empath Feature Extractor\n",
        "class EmpathFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, documents, labels, selected_categories, output_folder=\"data/feature_extracted_data\"):\n",
        "        super().__init__(documents, labels, output_folder)\n",
        "        self.lexicon = Empath()\n",
        "        self.categories = selected_categories\n",
        "        self.features = None\n",
        "        self.correlation_results = None\n",
        "        self.significant_results = None\n",
        "\n",
        "    def extract_empath_features(self):\n",
        "        features = []\n",
        "        for doc in self.documents:\n",
        "            doc_features = {}\n",
        "\n",
        "            # Linguistic features\n",
        "            for category in self.categories.get(\"linguistic_features\", []):\n",
        "                doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            # Psychological processes\n",
        "            for subcategory, subcategories in self.categories.get(\"psychological_processes\", {}).items():\n",
        "                for category in subcategories:\n",
        "                    doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            # Personal concerns\n",
        "            for category in self.categories.get(\"personal_concerns\", []):\n",
        "                doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            # Time orientations\n",
        "            for category in self.categories.get(\"time_orientations\", []):\n",
        "                doc_features[category] = self.lexicon.analyze(doc, categories=[category])[category]\n",
        "\n",
        "            features.append(doc_features)\n",
        "\n",
        "        # Convert to a DataFrame\n",
        "        self.features = pd.DataFrame(features)\n",
        "\n",
        "        # Add labels to the features DataFrame\n",
        "        if len(self.features) == len(self.labels):\n",
        "            self.features['label'] = self.labels\n",
        "            print(\"Added label column to the extracted features.\")\n",
        "        else:\n",
        "            raise ValueError(\"Mismatch between the number of features and labels.\")\n",
        "\n",
        "        print(f\"Extracted Empath features with shape: {self.features.shape}\")\n",
        "\n",
        "    def analyze_correlation(self):\n",
        "        if self.features is None:\n",
        "            raise ValueError(\"Features must be extracted before analyzing correlations.\")\n",
        "\n",
        "        # Remove constant columns\n",
        "        constant_columns = self.features.columns[self.features.nunique() == 1]\n",
        "        self.features.drop(columns=constant_columns, inplace=True, errors='ignore')\n",
        "        print(f\"Removed constant columns: {list(constant_columns)}\")\n",
        "\n",
        "        # Validate labels\n",
        "        if len(set(self.labels)) == 1:\n",
        "            raise ValueError(\"Labels array is constant; cannot compute correlation.\")\n",
        "\n",
        "        correlations, p_values = [], []\n",
        "\n",
        "        for column in self.features.columns.drop(\"label\"):\n",
        "            correlation, p_value = pearsonr(self.features[column], self.labels)\n",
        "            correlations.append(correlation)\n",
        "            p_values.append(p_value)\n",
        "\n",
        "        correction_results = multipletests(p_values, alpha=0.05, method=\"fdr_bh\")\n",
        "        _, corrected_p_values, _, _ = correction_results\n",
        "\n",
        "        # Create a correlation DataFrame\n",
        "        self.correlation_results = pd.DataFrame({\n",
        "            \"Feature\": self.features.columns.drop(\"label\"),\n",
        "            \"Correlation\": correlations,\n",
        "            \"P-Value\": p_values,\n",
        "            \"Corrected P-Value\": corrected_p_values\n",
        "        }).sort_values(by=\"Correlation\", key=abs, ascending=False)\n",
        "\n",
        "    def save_features_and_results(self):\n",
        "        if self.features is not None:\n",
        "            feature_file = os.path.join(self.output_folder, \"empath_features_with_labels.csv\")\n",
        "            if not os.path.exists(feature_file):\n",
        "                self.features.to_csv(feature_file, index=False)\n",
        "                print(f\"Saved empath features with labels to {feature_file}.\")\n",
        "            else:\n",
        "                print(f\"Empath features file already exists at {feature_file}.\")\n",
        "\n",
        "        if self.correlation_results is not None:\n",
        "            correlation_file = os.path.join(self.output_folder, \"empath_correlation_results.csv\")\n",
        "            if not os.path.exists(correlation_file):\n",
        "                self.correlation_results.to_csv(correlation_file, index=False)\n",
        "                print(f\"Saved correlation results to {correlation_file}.\")\n",
        "            else:\n",
        "                print(f\"Correlation results file already exists at {correlation_file}.\")\n",
        "\n",
        "\n",
        "# Empath Feature Analyzer\n",
        "class EmpathFeatureAnalyzer(EmpathFeatureExtractor):\n",
        "    def __init__(self, documents, labels, categories, output_folder=\"data/feature_extracted_data\"):\n",
        "        super().__init__(documents, labels, categories, output_folder)\n",
        "        self.category_correlations = {}\n",
        "\n",
        "    def group_correlations_by_subcategory(self):\n",
        "        \"\"\"\n",
        "        Group feature correlations by subcategories and calculate the average correlation for each group.\n",
        "        \"\"\"\n",
        "        if self.correlation_results is None:\n",
        "            raise ValueError(\"Correlation analysis must be performed before grouping.\")\n",
        "\n",
        "        grouped_results = defaultdict(list)\n",
        "\n",
        "        # Iterate through features, correlations, and P-values\n",
        "        for feature, correlation, p_value in zip(\n",
        "            self.correlation_results['Feature'],\n",
        "            self.correlation_results['Correlation'],\n",
        "            self.correlation_results['P-Value']\n",
        "        ):\n",
        "            for category, subcategories in self.categories.items():\n",
        "                if isinstance(subcategories, dict):  # Handle nested subcategories\n",
        "                    for subcategory, sub_features in subcategories.items():\n",
        "                        if feature in sub_features:\n",
        "                            grouped_results[f\"{category} - {subcategory}\"].append((feature, correlation, p_value))\n",
        "                elif isinstance(subcategories, list):  # Handle flat categories\n",
        "                    if feature in subcategories:\n",
        "                        grouped_results[category].append((feature, correlation, p_value))\n",
        "\n",
        "        # Store the top example words and average correlations\n",
        "        self.category_correlations = {}\n",
        "        for group, correlations in grouped_results.items():\n",
        "            avg_correlation = np.mean([c[1] for c in correlations])\n",
        "            sorted_features = sorted(correlations, key=lambda x: abs(x[1]), reverse=True)\n",
        "            top_feature = sorted_features[0]  # Select the most correlated feature as an example\n",
        "            self.category_correlations[group] = {\n",
        "                \"Example Word\": top_feature[0],\n",
        "                \"Correlation\": avg_correlation,\n",
        "                \"P-Value\": top_feature[2]\n",
        "            }\n",
        "\n",
        "    def generate_summary_table(self):\n",
        "        \"\"\"\n",
        "        Generate a summary table with categories, example words, correlations, and P-values.\n",
        "        \"\"\"\n",
        "        if not self.category_correlations:\n",
        "            raise ValueError(\"Category correlations must be computed before generating a summary table.\")\n",
        "\n",
        "        summary_data = []\n",
        "        for category, details in self.category_correlations.items():\n",
        "            summary_data.append({\n",
        "                \"LIWC Category\": category,\n",
        "                \"Example Word\": details[\"Example Word\"],\n",
        "                \"Correlation\": f\"{details['Correlation']:.2f}\",\n",
        "                \"P-Value\": f\"{details['P-Value']:.3f}\"\n",
        "            })\n",
        "\n",
        "        summary_df = pd.DataFrame(summary_data)\n",
        "        return summary_df\n",
        "\n",
        "    def visualize_summary_table(self):\n",
        "        \"\"\"\n",
        "        Display the summary table in the notebook.\n",
        "        \"\"\"\n",
        "        summary_table = self.generate_summary_table()\n",
        "        print(summary_table.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LDA Feature Extractor\n",
        "class LDAFeatureExtractor(FeatureExtractor):\n",
        "    def __init__(self, documents, labels, num_topics=70, passes=15, output_folder=\"data/feature_extracted_data\", random_state=42):\n",
        "        super().__init__(documents, labels, output_folder)\n",
        "        self.num_topics = num_topics\n",
        "        self.passes = passes\n",
        "        self.random_state = random_state\n",
        "        self.dictionary = None\n",
        "        self.corpus = None\n",
        "        self.lda_model = None\n",
        "        self.topic_distributions = None\n",
        "        self.tsne_results = None\n",
        "\n",
        "    def preprocess_documents(self):\n",
        "        \"\"\"\n",
        "        Preprocess documents: tokenize, remove stopwords, and stem.\n",
        "        \"\"\"\n",
        "        stop_words = set(stopwords.words('english'))\n",
        "        stemmer = PorterStemmer()\n",
        "        processed_docs = [\n",
        "            [\n",
        "                stemmer.stem(word) for word in word_tokenize(doc.lower())\n",
        "                if word.isalpha() and word not in stop_words\n",
        "            ]\n",
        "            for doc in self.documents\n",
        "        ]\n",
        "        return processed_docs\n",
        "\n",
        "    def filter_docs_by_word_count(self, processed_docs, min_documents=10):\n",
        "        \"\"\"\n",
        "        Filter words that appear in more than 10 documents.\n",
        "        \"\"\"\n",
        "        word_doc_count = defaultdict(int)\n",
        "        for doc in processed_docs:\n",
        "            unique_words = set(doc)\n",
        "            for word in unique_words:\n",
        "                word_doc_count[word] += 1\n",
        "\n",
        "        filtered_docs = [\n",
        "            [word for word in doc if word_doc_count[word] > min_documents]\n",
        "            for doc in processed_docs\n",
        "        ]\n",
        "        return filtered_docs\n",
        "\n",
        "    def train_lda(self, processed_docs):\n",
        "        \"\"\"\n",
        "        Train the LDA model.\n",
        "        \"\"\"\n",
        "        self.dictionary = corpora.Dictionary(processed_docs)\n",
        "        self.corpus = [self.dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "        self.lda_model = LdaModel(\n",
        "            self.corpus, num_topics=self.num_topics, id2word=self.dictionary, passes=self.passes,\n",
        "            random_state=self.random_state\n",
        "        )\n",
        "\n",
        "    def extract_topic_distributions(self):\n",
        "        \"\"\"\n",
        "        Extract topic distributions for each document.\n",
        "        \"\"\"\n",
        "        self.topic_distributions = [\n",
        "            dict(self.lda_model.get_document_topics(doc, minimum_probability=0))\n",
        "            for doc in self.corpus\n",
        "        ]\n",
        "\n",
        "    def topic_distribution_to_matrix(self):\n",
        "        \"\"\"\n",
        "        Convert topic distributions to a matrix format.\n",
        "        \"\"\"\n",
        "        matrix = np.zeros((len(self.topic_distributions), self.num_topics))\n",
        "        for i, distribution in enumerate(self.topic_distributions):\n",
        "            for topic_id, prob in distribution.items():\n",
        "                matrix[i, topic_id] = prob\n",
        "        return matrix\n",
        "\n",
        "    def visualize_lda(self, label_filter=None):\n",
        "        \"\"\"\n",
        "        Visualize the LDA model with pyLDAvis, with an option to filter by depressed or non-depressed posts.\n",
        "\n",
        "        Parameters:\n",
        "        label_filter (int, optional): Filter by label. Use 1 for depressed, 0 for non-depressed. \n",
        "                                       If None, visualize all posts.\n",
        "        \"\"\"\n",
        "        if not self.lda_model or not self.dictionary:\n",
        "            raise ValueError(\"LDA model or dictionary not available. Train the model first.\")\n",
        "        \n",
        "        # Filtered visualization\n",
        "        if label_filter is not None:\n",
        "            filtered_docs = [doc for doc, label in zip(self.documents, self.labels) if label == label_filter]\n",
        "            print(f\"Generating LDA visualization for {'depressed' if label_filter == 1 else 'non-depressed'} posts...\")\n",
        "\n",
        "            # Preprocess the filtered documents\n",
        "            stop_words = set(stopwords.words('english'))\n",
        "            stemmer = PorterStemmer()\n",
        "            processed_docs = [\n",
        "                [\n",
        "                    stemmer.stem(word) for word in word_tokenize(doc.lower())\n",
        "                    if word.isalpha() and word not in stop_words\n",
        "                ]\n",
        "                for doc in filtered_docs\n",
        "            ]\n",
        "            \n",
        "            # Create a corpus for the filtered documents\n",
        "            filtered_corpus = [self.dictionary.doc2bow(doc) for doc in processed_docs]\n",
        "            vis = gensimvis.prepare(self.lda_model, filtered_corpus, self.dictionary)\n",
        "        else:\n",
        "            # Visualization for all posts\n",
        "            print(\"Generating LDA visualization for all posts...\")\n",
        "            vis = gensimvis.prepare(self.lda_model, self.corpus, self.dictionary)\n",
        "\n",
        "        # Ensure pyLDAvis works in the notebook\n",
        "        pyLDAvis.enable_notebook()  # Enable visualization in Jupyter Notebook\n",
        "\n",
        "        # Display visualization\n",
        "        return pyLDAvis.display(vis)\n",
        "\n",
        "    def visualize_tsne(self, matrix):\n",
        "        \"\"\"\n",
        "        Visualize topic distributions using t-SNE.\n",
        "        \"\"\"\n",
        "        labels = self.labels\n",
        "        tsne = TSNE(n_components=2, perplexity=50, n_iter=500, random_state=self.random_state)\n",
        "        self.tsne_results = tsne.fit_transform(matrix)\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(10, 7))\n",
        "        scatter = plt.scatter(\n",
        "            self.tsne_results[:, 0], self.tsne_results[:, 1], c=labels, cmap='viridis', s=10, alpha=0.7\n",
        "        )\n",
        "        plt.legend(handles=scatter.legend_elements()[0], labels=[\"Non-Depressed\", \"Depressed\"], title=\"Labels\")\n",
        "        plt.title(\"t-SNE Visualization of LDA Topic Distributions\")\n",
        "        plt.xlabel(\"Dimension 1\")\n",
        "        plt.ylabel(\"Dimension 2\")\n",
        "        plt.show()\n",
        "\n",
        "    def save_features(self):\n",
        "        \"\"\"\n",
        "        Save LDA topic distributions with labels to a CSV file.\n",
        "        \"\"\"\n",
        "        if not self.topic_distributions:\n",
        "            raise ValueError(\"Topic distributions are not extracted.\")\n",
        "\n",
        "        # Prepare the LDA features data\n",
        "        topic_matrix = self.topic_distribution_to_matrix()\n",
        "        labels_array = np.array(self.labels)\n",
        "        lda_features_df = pd.DataFrame(topic_matrix)\n",
        "        lda_features_df['label'] = labels_array\n",
        "\n",
        "        # Define the filename\n",
        "        lda_features_file = \"lda_topic_distributions_with_labels.csv\"\n",
        "\n",
        "        # Call the base class method for saving\n",
        "        self.save_to_csv(lda_features_df, lda_features_file)\n",
        "        \n",
        "    def run_pipeline(self):\n",
        "        \"\"\"\n",
        "        Complete LDA pipeline: preprocess, train, extract, visualize, and save.\n",
        "        \"\"\"\n",
        "        print(\"Preprocessing documents...\")\n",
        "        processed_docs = self.preprocess_documents()\n",
        "        filtered_docs = self.filter_docs_by_word_count(processed_docs)\n",
        "\n",
        "        print(\"Training LDA model...\")\n",
        "        self.train_lda(filtered_docs)\n",
        "\n",
        "        print(\"Extracting topic distributions...\")\n",
        "        self.extract_topic_distributions()\n",
        "\n",
        "        print(\"Saving features...\")\n",
        "        self.save_features()\n",
        "\n",
        "        print(\"Visualizing t-SNE...\")\n",
        "        topic_matrix = self.topic_distribution_to_matrix()\n",
        "        self.visualize_tsne(topic_matrix)\n",
        "\n",
        "        print(\"LDA pipeline complete.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded 1878 documents.\n",
            "Labels: {0, 1}\n"
          ]
        }
      ],
      "source": [
        "# Loading data\n",
        "if __name__ == \"__main__\":\n",
        "    # Load documents and labels\n",
        "    folders = {\n",
        "        \"depression\": {\"path\": \"data/preprocessed_posts/depression\", \"label\": 1},\n",
        "        \"breastcancer\": {\"path\": \"data/preprocessed_posts/breastcancer\", \"label\": 0},\n",
        "    }\n",
        "    documents, labels = [], []\n",
        "    for category, data in folders.items():\n",
        "        for file_name in os.listdir(data[\"path\"]):\n",
        "            file_path = os.path.join(data[\"path\"], file_name)\n",
        "            with open(file_path, 'r', encoding='utf-8') as file:\n",
        "                documents.append(file.read())\n",
        "                labels.append(data[\"label\"])\n",
        "    print(f\"Loaded {len(documents)} documents.\")\n",
        "    print(f\"Labels: {set(labels)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# N-Gram Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize, extract, save, analyze, train and generate word clouds.\n",
        "# Initialize the n-gram extractor\n",
        "ngram_extractor = NGramFeatureExtractor(documents, labels)\n",
        "\n",
        "# Extract features\n",
        "ngram_extractor.extract_features()\n",
        "\n",
        "# Save features\n",
        "ngram_extractor.save_features()\n",
        "\n",
        "# Analyze top features\n",
        "print(\"\\nTop Unigram Features:\")\n",
        "ngram_extractor.get_top_features(feature_type=\"unigram\", top_n=10)\n",
        "print(\"\\nTop Bigram Features:\")\n",
        "ngram_extractor.get_top_features(feature_type=\"bigram\", top_n=10)\n",
        "\n",
        "\n",
        "# Train a model\n",
        "print(\"\\nTraining a model with unigram features...\")\n",
        "classifier = ngram_extractor.train_model(feature_type=\"unigram\")\n",
        "\n",
        "# Generate word clouds\n",
        "print(\"\\nGenerating Word Clouds...\")\n",
        "ngram_extractor.generate_wordclouds()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Empath Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stored 'selected_categories' (dict)\n"
          ]
        }
      ],
      "source": [
        "# Categories to focus on based on the origin paper. \n",
        "# The categories are divided into linguistic features, psychological processes, personal concerns, and time orientations.\n",
        "# Since we cannot use the LIWC tool, we will use the Empath tool and define similar categorical features. \n",
        "\n",
        "selected_categories = {\n",
        "    \"linguistic_features\": [\n",
        "        \"articles\", \"auxiliary_verbs\", \"adverbs\", \"conjunctions\", \n",
        "        \"personal_pronouns\", \"impersonal_pronouns\", \"negations\", \n",
        "        \"prepositions\", \"verbs\", \"nouns\", \"adjectives\", \n",
        "        \"comparatives\", \"superlatives\", \"modifiers\", \"function_words\", \n",
        "        \"filler_words\", \"verb_tense\", \"slang\", \"jargon\", \n",
        "        \"formal_language\", \"casual_language\", \"exclamations\", \n",
        "        \"contractions\", \"word_complexity\", \"sentiment_words\"\n",
        "    ],\n",
        "        \"psychological_processes\": {\n",
        "        \"affective\": [\n",
        "            \"positive_emotion\", \"negative_emotion\", \"joy\", \"anger\", \n",
        "            \"sadness\", \"anxiety\", \"fear\", \"disgust\", \"love\", \n",
        "            \"hope\", \"trust\", \"excitement\", \"anticipation\", \n",
        "            \"relief\", \"sympathy\", \"gratitude\", \"shame\", \n",
        "            \"guilt\", \"envy\", \"pride\", \"contentment\", \"confusion\",\n",
        "            \"boredom\", \"embarrassment\", \"longing\", \"nostalgia\", \n",
        "            \"embarrassment\", \"frustration\", \"surprise\", \"melancholy\"\n",
        "        ],\n",
        "        \"biological\": [\n",
        "            \"body\", \"health\", \"illness\", \"pain\", \"hygiene\", \n",
        "            \"fitness\", \"exercise\", \"nutrition\", \"ingestion\", \n",
        "            \"physical_state\", \"medicine\", \"sleep\", \"sexual\", \n",
        "            \"aging\", \"disease\", \"injury\", \"hospital\", \"recovery\", \n",
        "            \"dieting\", \"mental_health\", \"drug_use\", \"headache\", \n",
        "            \"fatigue\", \"hormones\", \"appetite\"\n",
        "        ],\n",
        "        \"social\": [\n",
        "            \"family\", \"friends\", \"relationships\", \"group_behavior\", \n",
        "            \"teamwork\", \"social_media\", \"communication\", \"community\", \n",
        "            \"peer_pressure\", \"leadership\", \"parenting\", \"mentorship\", \n",
        "            \"marriage\", \"divorce\", \"gender_roles\", \"social_identity\", \n",
        "            \"cultural_rituals\", \"networking\", \"altruism\", \"conflict\", \n",
        "            \"social_support\", \"dominance\", \"affiliation\", \"intimacy\", \n",
        "            \"supportiveness\", \"competition\", \"conflict_resolution\", \n",
        "            \"collaboration\", \"in-group\", \"out-group\", \"prejudice\"\n",
        "        ],\n",
        "        \"cognitive\": [\n",
        "            \"certainty\", \"doubt\", \"insight\", \"cause\", \"discrepancy\", \n",
        "            \"problem_solving\", \"creativity\", \"self_reflection\", \"planning\", \n",
        "            \"memory\", \"perception\", \"attention\", \"reasoning\", \"thought_process\", \n",
        "            \"decision_making\", \"confusion\", \"learning\", \"metacognition\", \"adaptability\", \n",
        "            \"focus\", \"perspective\", \"problem_analysis\", \"evaluation\", \"interpretation\",\n",
        "            \"logic\", \"intelligence\", \"rational_thought\", \"intuition\", \"conceptualization\"\n",
        "        ],\n",
        "        \"drives\": [\n",
        "            \"achievement\", \"dominance\", \"affiliation\", \"control\", \n",
        "            \"self-esteem\", \"autonomy\", \"self-assertion\", \"power\", \n",
        "            \"ambition\", \"conformity\", \"subordination\", \"dependence\", \n",
        "            \"submission\", \"accomplishment\", \"independence\", \"order\", \n",
        "            \"control_seeking\", \"status\", \"prosocial_behavior\"\n",
        "        ],\n",
        "        \"spiritual\": [\n",
        "            \"spirituality\", \"faith\", \"beliefs\", \"sacred\", \"religion\", \n",
        "            \"prayer\", \"meditation\", \"afterlife\", \"soul\", \"divine\", \n",
        "            \"god\", \"higher_power\", \"inspiration\", \"transcendence\", \n",
        "            \"morality\", \"ethics\", \"rituals\", \"holiness\", \"mindfulness\"\n",
        "        ]\n",
        "    },\n",
        "    \"personal_concerns\": [\n",
        "        \"work\", \"money\", \"wealth\", \"shopping\", \"career\", \"travel\", \n",
        "        \"home\", \"school\", \"education\", \"violence\", \"death\", \n",
        "        \"retirement\", \"spirituality\", \"family_life\", \"hobbies\", \n",
        "        \"volunteering\", \"pets\", \"entertainment\", \"parenting\", \n",
        "        \"sports\", \"adventure\", \"politics\", \"environment\", \n",
        "        \"safety\", \"technology\", \"materialism\", \"status\", \n",
        "        \"self_improvement\", \"learning\", \"self_growth\", \"happiness\", \n",
        "        \"life_purpose\", \"work_life_balance\", \"stress\", \"coping\", \n",
        "        \"job_satisfaction\", \"ambition\", \"legacy\", \"job_search\", \n",
        "        \"unemployment\", \"retirement_plans\", \"mental_health\", \"dating\", \n",
        "        \"romantic_relationships\", \"divorce\", \"life_stressors\", \"transitions\"\n",
        "    ],\n",
        "    \"time_orientations\": [\n",
        "        \"present\", \"past\", \"future\", \"morning\", \n",
        "        \"afternoon\", \"evening\", \"day\", \"night\", \n",
        "        \"weekdays\", \"weekends\", \"seasons\", \"holidays\", \n",
        "        \"lifespan\", \"long_term\", \"short_term\", \n",
        "        \"routine\", \"historical\", \"epoch\", \"momentary\", \n",
        "        \"timeliness\", \"timelessness\", \"urgency\", \n",
        "        \"progression\", \"nostalgia\", \"anticipation\"\n",
        "    ]\n",
        "}\n",
        "%store selected_categories"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added label column to the extracted features.\n",
            "Extracted Empath features with shape: (1878, 238)\n",
            "Removed constant columns: ['articles', 'auxiliary_verbs', 'adverbs', 'conjunctions', 'personal_pronouns', 'impersonal_pronouns', 'negations', 'prepositions', 'verbs', 'nouns', 'adjectives', 'comparatives', 'superlatives', 'modifiers', 'function_words', 'filler_words', 'verb_tense', 'slang', 'jargon', 'formal_language', 'casual_language', 'exclamations', 'contractions', 'word_complexity', 'sentiment_words', 'anxiety', 'hope', 'excitement', 'relief', 'gratitude', 'guilt', 'boredom', 'embarrassment', 'longing', 'nostalgia', 'frustration', 'melancholy', 'illness', 'fitness', 'nutrition', 'ingestion', 'physical_state', 'medicine', 'aging', 'disease', 'hospital', 'recovery', 'dieting', 'mental_health', 'drug_use', 'headache', 'fatigue', 'hormones', 'appetite', 'relationships', 'group_behavior', 'teamwork', 'community', 'peer_pressure', 'leadership', 'parenting', 'mentorship', 'marriage', 'divorce', 'gender_roles', 'social_identity', 'cultural_rituals', 'networking', 'altruism', 'conflict', 'social_support', 'dominance', 'affiliation', 'intimacy', 'supportiveness', 'competition', 'conflict_resolution', 'collaboration', 'in-group', 'out-group', 'prejudice', 'certainty', 'doubt', 'insight', 'cause', 'discrepancy', 'problem_solving', 'creativity', 'self_reflection', 'planning', 'memory', 'perception', 'attention', 'reasoning', 'thought_process', 'decision_making', 'learning', 'metacognition', 'adaptability', 'focus', 'perspective', 'problem_analysis', 'evaluation', 'interpretation', 'logic', 'intelligence', 'rational_thought', 'intuition', 'conceptualization', 'control', 'self-esteem', 'autonomy', 'self-assertion', 'ambition', 'conformity', 'subordination', 'dependence', 'submission', 'accomplishment', 'control_seeking', 'status', 'prosocial_behavior', 'spirituality', 'faith', 'beliefs', 'sacred', 'prayer', 'meditation', 'afterlife', 'soul', 'god', 'higher_power', 'inspiration', 'transcendence', 'morality', 'ethics', 'rituals', 'holiness', 'mindfulness', 'wealth', 'career', 'travel', 'education', 'retirement', 'family_life', 'hobbies', 'volunteering', 'pets', 'entertainment', 'adventure', 'environment', 'safety', 'materialism', 'self_improvement', 'self_growth', 'happiness', 'life_purpose', 'work_life_balance', 'stress', 'coping', 'job_satisfaction', 'legacy', 'job_search', 'unemployment', 'retirement_plans', 'dating', 'romantic_relationships', 'life_stressors', 'transitions', 'present', 'past', 'future', 'afternoon', 'evening', 'day', 'weekdays', 'weekends', 'seasons', 'holidays', 'lifespan', 'long_term', 'short_term', 'routine', 'historical', 'epoch', 'momentary', 'timeliness', 'timelessness', 'urgency', 'progression']\n",
            "Saved empath features with labels to data/feature_extracted_data/empath_features_with_labels.csv.\n",
            "Saved correlation results to data/feature_extracted_data/empath_correlation_results.csv.\n"
          ]
        }
      ],
      "source": [
        "# Initialize Empath Feature Extractor\n",
        "empath_extractor = EmpathFeatureExtractor(documents, labels, selected_categories)\n",
        "\n",
        "# Extract features\n",
        "empath_extractor.extract_empath_features()\n",
        "\n",
        "# Analyze correlations\n",
        "empath_extractor.analyze_correlation()\n",
        "\n",
        "# Save features and results\n",
        "empath_extractor.save_features_and_results()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OBY4UR2DAEqD"
      },
      "source": [
        "# LDA (Latent Dirichlet Allocation) Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initializing LDA Feature Extractor\n",
        "lda_extractor = LDAFeatureExtractor(documents, labels)\n",
        "lda_extractor.run_pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualising depression\n",
        "lda_extractor.visualize_lda(label_filter=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualising non-depression\n",
        "lda_extractor.visualize_lda(label_filter=0)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyOSH8TbJAtGxRwhwTt6MCJf",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "aai",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
